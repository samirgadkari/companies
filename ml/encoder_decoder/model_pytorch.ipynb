{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:45:12.077427Z",
     "start_time": "2020-11-17T01:45:12.073543Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from glob import iglob\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from utils.environ import generated_data_dir\n",
    "from utils.file import read_file, get_json_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:04:56.509507Z",
     "start_time": "2020-11-16T17:04:56.498451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:04:57.110925Z",
     "start_time": "2020-11-16T17:04:56.513281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html_fn: /Volumes/Seagate/generated-data/html/0.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/0.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/1.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/1.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/2.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/2.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/3.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/3.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/4.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/4.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/5.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/5.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/6.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/6.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/7.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/7.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/8.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/8.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/9.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/9.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/10.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/10.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/11.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/11.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/12.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/12.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/13.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/13.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/14.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/14.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/15.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/15.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/16.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/16.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/17.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/17.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/18.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/18.expected_json\n",
      "html_fn: /Volumes/Seagate/generated-data/html/19.unescaped\n",
      "json_fn: /Volumes/Seagate/generated-data/expected_json/19.expected_json\n"
     ]
    }
   ],
   "source": [
    "# This script tokenizes the files in these directories:\n",
    "#   HTML files: /Volumes/Seagate/generated-data/html/*.unescaped\n",
    "#   JSON files: /Volumes/Seagate/generated-data/expected_json/*.expected_json\n",
    "# and saves the tokens in the space-delimited files in the directory:\n",
    "#   HTML files: /Volumes/Seagate/generated-data/html/tokenized/*.unescaped\n",
    "#   JSON files: /Volumes/Seagate/generated-data/expected_json/tokenized/*.expected_json\n",
    "%run preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T00:04:27.530149Z",
     "start_time": "2020-11-17T00:04:27.500447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting filenames ... done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3359"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run train_set_max_token_len.py\n",
    "\n",
    "def parse_max_token_len(filename):\n",
    "    max_token_len = read_file(filename).strip().split(':')[1].strip()\n",
    "    return int(max_token_len)\n",
    "\n",
    "max_encoded_file_token_len = parse_max_token_len(os.path.join(generated_data_dir(), 'max_token_len'))\n",
    "max_encoded_file_token_len  # This is the maximum number of tokens in any html/json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T00:04:34.097887Z",
     "start_time": "2020-11-17T00:04:34.090898Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.environ import generated_data_dir, tables_extracted_split_tables_dir\n",
    "\n",
    "class TransformerDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, type_='train'):\n",
    "        super(TransformerDataset).__init__()\n",
    "        if type_ == 'train':  # training data\n",
    "            tokenized_fn = os.path.join(generated_data_dir(), 'tokenized')\n",
    "        else:                 # testing data\n",
    "            tokenized_fn = os.path.join(tables_extracted_split_tables_dir(), 'tokenized')\n",
    "            \n",
    "        self.file_handle = open(tokenized_fn, 'r')\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Return a generator. A generator is also an iterator.\n",
    "        def gen():\n",
    "            for line in self.file_handle:\n",
    "                parts = line.rstrip('\\n').split('^')\n",
    "                t = (parts[1].split(), parts[3].split())\n",
    "                yield t\n",
    "                    \n",
    "        return iter(gen())\n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "    overall_start = dataset.start\n",
    "    overall_end = dataset.end\n",
    "    # configure the dataset to only process the split workload\n",
    "    per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "    worker_id = worker_info.id\n",
    "    dataset.start = overall_start + worker_id * per_worker\n",
    "    dataset.end = min(dataset.start + per_worker, overall_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:51:10.980821Z",
     "start_time": "2020-11-17T01:51:10.974599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<sos>': 0, '<pad>': 1, '<eos>': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '10': 13, '11': 14, '12': 15, '13': 16, '14': 17, '15': 18, '16': 19, '17': 20, '18': 21, '19': 22, '20': 23, '21': 24, '22': 25, '23': 26, '24': 27, '25': 28, '26': 29, '27': 30, '28': 31, '29': 32, '30': 33, '31': 34, '32': 35, '33': 36, '34': 37, '35': 38, '36': 39, '37': 40, '38': 41, '39': 42, '40': 43, '41': 44, '42': 45, '43': 46, '44': 47, '45': 48, '46': 49, '47': 50, '48': 51, '49': 52, '50': 53, '51': 54, '52': 55, '53': 56, '54': 57, '55': 58, '56': 59, '57': 60, '58': 61, '59': 62, '60': 63, '61': 64, '62': 65, '63': 66, '64': 67, '65': 68, '66': 69, '67': 70, '68': 71, '69': 72, '70': 73, '71': 74, '72': 75, '73': 76, '74': 77, '75': 78, '76': 79, '77': 80, '78': 81, '79': 82, '80': 83, '81': 84, '82': 85, '83': 86, '84': 87, '85': 88, '86': 89, '87': 90, '88': 91, '89': 92, '90': 93, '91': 94, '92': 95, '93': 96, '94': 97, '95': 98, '96': 99, '97': 100, '98': 101, '99': 102, '100': 103, '-': 104, 'a': 105, 'b': 106, 'big': 107, 'br': 108, 'body': 109, 'c': 110, 'caption': 111, 'center': 112, 'dd': 113, 'dl': 114, 'dir': 115, 'div': 116, 'dt': 117, 'em': 118, 'f1': 119, 'f2': 120, 'f3': 121, 'f4': 122, 'f5': 123, 'f6': 124, 'f7': 125, 'f8': 126, 'f9': 127, 'f10': 128, 'f11': 129, 'fn': 130, 'font': 131, 'h1': 132, 'h2': 133, 'h3': 134, 'h4': 135, 'h5': 136, 'h6': 137, 'hr': 138, 'html': 139, 'i': 140, 'ix:nonfraction': 141, 'ix:nonnumeric': 142, 'img': 143, 'li': 144, 'nobr': 145, 'p': 146, 'page': 147, 's': 148, 'small': 149, 'span': 150, 'sup': 151, 'sub': 152, 'strong': 153, 't': 154, 'table': 155, 'td': 156, 'th': 157, 'tr': 158, 'u': 159, 'ul': 160, 'document': 161, 'end_a': 162, 'end_b': 163, 'end_big': 164, 'end_br': 165, 'end_body': 166, 'end_c': 167, 'end_caption': 168, 'end_center': 169, 'end_dd': 170, 'end_dl': 171, 'end_dir': 172, 'end_div': 173, 'end_dt': 174, 'end_em': 175, 'end_f1': 176, 'end_f2': 177, 'end_f3': 178, 'end_f4': 179, 'end_f5': 180, 'end_f6': 181, 'end_f7': 182, 'end_f8': 183, 'end_f9': 184, 'end_f10': 185, 'end_f11': 186, 'end_fn': 187, 'end_font': 188, 'end_h1': 189, 'end_h2': 190, 'end_h3': 191, 'end_h4': 192, 'end_h5': 193, 'end_h6': 194, 'end_hr': 195, 'end_html': 196, 'end_i': 197, 'end_ix:nonfraction': 198, 'end_ix:nonnumeric': 199, 'end_img': 200, 'end_li': 201, 'end_nobr': 202, 'end_p': 203, 'end_page': 204, 'end_s': 205, 'end_small': 206, 'end_span': 207, 'end_sup': 208, 'end_sub': 209, 'end_strong': 210, 'end_t': 211, 'end_table': 212, 'end_td': 213, 'end_th': 214, 'end_tr': 215, 'end_u': 216, 'end_ul': 217, 'end_document': 218, 'rowspan': 219, 'colspan': 220, 'style': 221, 'align': 222, 'width': 223, 'text-align': 224, '{': 225, '}': 226, '[': 227, ']': 228, ':': 229, '\"': 230, ',': 231, 'name': 232, 'values': 233, 'header': 234, 'table_data': 235, 'LKYUB9Vakp8Sap0': 236, 'USssrfBozhlO': 237, 'cmJaEUQG4u8BPPoKX56H': 238, 'cCVY199XfC': 239, 'jXFztBavB7mufPtwGs': 240, 'flchDJm': 241, 'p7r3zURdIURR2oFU': 242, 'tLp38LM': 243, '1UU8hfS3k': 244, '6a8nwkTitgbGPOHX3': 245, '4itjMVLNrxGpmmss': 246, '00e0xcvN': 247, 'sfr3CkZtrSseKplOOdB': 248, '1jIowCYibqhdMnLW': 249, 'f0FPAYJMKZlxA1j': 250, 'boCQ1EZPlCQ3RaC': 251, 'tF4VHUK5172jCi': 252, 't9emSjny2Dh': 253, 'YsIFb': 254, '2Z5ijycG80T': 255, 'vhxowaHD': 256, '8poYOtd': 257, 'dyBuIUZgU2': 258, 'XuLc0E8KkDGuJoxcDCEL': 259, 'ycwyr': 260, 'WEOtiZpnfdSugWYnqj': 261, '7GwjM49rJF43J3dNDwJC': 262, '9pZKWmT4k8VpfonjixSd': 263, 'lF0l793': 264, 'b1Rai': 265, 'UHonBops1StsZ61yHO': 266, 'Tro5MdN': 267, 'k6NDarQvcjXLfc3jA': 268, 'whgVKQs1K9TGDP': 269, 'LRnCO': 270, 'pIZXGcfYS': 271, 'p0srcJ7Z3CgJwA3b7VE': 272, '3UqGaMon5YqQ62Cwmqm': 273, 'HclnnkyCFFkCne7t': 274, 'kq3Ijnk7': 275, 'Ysqm5obJCDsokcxhz3Po': 276, 'wiHbDwFy16fnq8w': 277, 'BwaCfxf8HmYoFhjDhD': 278, 'WHmNdPYyTQ': 279, 'ZVUhD1TLvD': 280, 'KWtfHXBS': 281, 'eAshxC3UMnUPpm4': 282, 'pVWKtoqVhxC2ehCYE3oF': 283, 'ByYHpe': 284, '9z7hy4llB': 285, 'dK2ipy6l8ixpMKh': 286, 'fpA4dUUOs2ORLM': 287, 'i4NvHitch': 288, 'rrnYr0CWu88bK': 289, '0QBCIHRzNHqB': 290, 'xTRJJgzQ': 291, '38mF3C42cGaHK8': 292, '7ZoiGZB9br6jGBN92': 293, 'ilrQjL4z': 294, 'r4pYpsFjMiL8vVCzeN6': 295, 'fFarKOS': 296, '6T5pk5uEqQ2ISa': 297, 'LQizvk': 298, 'zf7tidu': 299, '7BSCH': 300, 'NZZFxxYddbmQDgNTB': 301, 'STFe0midV9aq3wj': 302, 'u6XZHSh': 303, 'QAF2ABJYbOSB': 304, 'HPn0A': 305, 'XTI5JHoXbVG': 306, '91v9ll67OApvI9QSQ': 307, 'MRkxqd': 308, '8eK80Wd4Ysb': 309, 'NmuJfdkxFfXvh4vY7': 310, 'ecWiKagr5Ns': 311, 'AStAxL1o89WxD': 312, 'sOYEQcztD4Fe0G': 313, 'fgAEN4D': 314, 'nm216Yg7bTr': 315, 'nvy8pYQl5CiylUV': 316, 'SjneYQ3EUb3YXv7Q2K': 317, 'dKg0VNdJE': 318, 'hvhCvk6': 319, 'YF0shaf1fCIKJN': 320, 'XIZcjfssrX': 321, '15bw3rDbKMIuf': 322, 'Esx4ub7MYonS8BcE': 323, 'F3qHBAzKd9P': 324, 'UxFuKl47TYRgcLfPeM': 325, '2fpFwoiL3AtYN': 326, 'INnHtiJFU7HMk9SPC': 327, 'MX8H3dPvPGD3Xl': 328, '7U7SNjrhbcjLXgei': 329, 'bSK3fsMOifXVROeIgzu': 330, 'WpZUwbMMp': 331, '3T7tKGT05h': 332, 'cSIqYm6': 333, '2lLLEG8hhaFv': 334, 'hFNKz50J': 335, 'oxev57N721pIGG8t': 336, 'P7KmALAnjezf0QQs': 337, 'ZxpoYkdOQfXP9568': 338, 'htUngQ': 339, 'kLDau': 340, '13Np0mR7OdzEfiBzQi': 341, '3BmiUu': 342, '7yCtYpG4AvWfkZex7Ib': 343, 'dPdJGwEprLXW': 344, 'MFFxFTAJfHEm6Y': 345, '4dspXezOWfOpc': 346, 'YNgEZlzUELfS3iO': 347, 'KOaiXFRr0HNe': 348, 'VNcBl7ZEcnaZ0oB': 349, 'rR8092zNZ': 350, '4EDQF35WshI6vjKDQ0': 351, 'bXka19fuRWgz': 352, 'wci89a2IYDafmrBOvcI': 353, '3qjc9IhHp': 354, 'ayJhgAUyiBl3sbyk': 355, 'LnTP6qNYjAVh0': 356, 'EJ00LaAoWXNOQjjvTe': 357, 'qNOxqZ6yT50IDNQ': 358, 'd2DPJ6Jrk7BKW4D5M': 359, 'sZZRTtT5JARADAo': 360, '2Y0vhLK249c3EG': 361, 'VaaP5MLvaj': 362, 'CmsITBbs04jn': 363, 'qYYFTCQYFfp': 364, 'z0Vcdx0OSgt6ZhRWqzG': 365, 'Y4Ujp': 366, 'ISrKAsh8OKbp': 367, 'HOmsx7QXhyY': 368, '67wFfdnvNwgV3NpHesjf': 369, 'hTsXKECRF': 370, 'ZMjIxGEtxS98': 371, 'T3YswVWI4JeeJdu4ndN': 372, 'YDTHmZqy': 373, 'a5xaQplXYAro': 374, 'Gr3ocQFEmRbDJ88C1u': 375, 'Ltheo67q591CPa2zVCV': 376, 'pMF73NpPmWh': 377, 'H6A4z24KZ': 378, 'xxHmiBP': 379, '493xIRNIqAK2MiuFA': 380, 'osJYAX2zOB7CT': 381, 'np7ZwR2': 382, 'HZMLtPkL20xtnFqjDYZc': 383, 'UAOK42WA': 384, 'bQAhxMIREV6D': 385, 'utdYyy6viDgr': 386, 'wcmBTnRGxlxxwvZO': 387, 'ctZjT06DhobP': 388, 'TbGqg': 389, 'qyaSkhEeinR': 390, 'uDqfhi0S4': 391, '6L4D7aM3EXzcoLNIWcI': 392, 'bIchpsI': 393, '8PlEZSiadbiEv3WKvE7': 394, 'fBOdxJtNMn2rwam': 395, '3m2Qv': 396, 'PvLB9': 397, 'YZiZAz4jE93CYP2': 398, 'j3GdUX9Kd6Zgl': 399, 'e8KbyKGj3lbpbsv': 400, 'HHQpu': 401, 'JR2s7rZRvIW': 402, 'ljX3HhnLyLYRk8': 403, 'GLSpTM': 404, 'bW8Zg9k': 405, 'QKC7TJZ': 406, 'gQY5VK4Z0tg7O8unp2Ia': 407, 'KhzM8ZKxe': 408, 'hvJpyeE2uTRw': 409, 'mvKieMzVA': 410, 'Jnh3kIYDWEcnMpUUGm8': 411, 'wLOZdIl': 412, 'XrUXjsPbJkcCAhkh2TS': 413, 'Blv4x4HHn7SYu1aRI': 414, 'nek95188gYlZBUI': 415, 'aEdvgle': 416, 'fxRl7IyxS9oQNaPO': 417, 'mVxIZvDyze': 418, 'pnXjqvDU': 419, '9WM7Xqat': 420, 'XSg23jLQHO': 421, 'ww47RptkRNc': 422, 'xAhequI': 423, 'QUx7PbS809PetVe': 424, 'eKI7PmkSbXeEeUk': 425, '5JlYTbIL6g2RMFMhp9': 426, 'maJvXyTNDhf48hA1F': 427, '8EagyFXVTiys0xFi': 428, '8nBKH5X8VZFj8jVKQ0': 429, 'bjEQZeowjPnTgc5iGlr': 430, 'ja5BZorbBER0ysmSH': 431, 'J3oGMUnMLeDzmSNq3Yc': 432, 'CzqEnGwds': 433, '1cJX2xaq': 434, '30RpHsWQDErHEZl242': 435, 'UtE92': 436, 'Mao7oZl': 437, 'Zny0wDqj7': 438, 'aRbOp': 439, 'KHMxGwZkldPFoCUM': 440, 'sWGzl9Ump': 441, 'TrjO9CtdCyRyvs5QG': 442, 'rzaTHbfcDWKK2KZwhM': 443, 'zsmpP9QgnM30': 444, 'GqWogqMT9kTc': 445, 'cMgZz4PkdE3cfhcw2': 446, 'EmmbBL1k2ghX9bi1': 447, 'tbNq2I': 448, 'aJsbZKfDrF': 449, '4mjqQYWK': 450, 'jbFn1g2u8gnahYBJ': 451, 'MsTMQ5BbwlO': 452, 'hcdvIJCmcV4tDJO': 453, 'pLrHTQcsNFO69kozJZA': 454, 'Bxq7fXhay': 455, '7qUBVVIA0ThUJkN': 456, '4gFWJU4yV': 457, 'CC3ELV1L4ZqiR': 458, 'isJsf2AzAxxuEX3': 459, 'x1ScUWf3hEO': 460, 'IsOFxGEhZAmK': 461, '5y7eU': 462, 'nxk0kvEnqf': 463, 'oO1D7T': 464, 'Zq5HhVjxI7bJPSBvZp8I': 465, 'djKgZEV': 466, 'f3qYcjf': 467, 'HS3dr2g': 468, 'pXHwxLnh': 469, '9NLnfqjN': 470, '98eVXo': 471, '30joAGH2lHyBuoEBBN1': 472, 'wkoiw7dc': 473, 'qpkYjCVNNXiGK5G': 474, 'yzniOvCxzbjtcI5kb1j': 475, 'z80vYsTKmEvvDlVE': 476, 'DXLLROno': 477, 'RhC5X': 478, 'QyThKMO4yDCQc5': 479, 'x2OWhVSk6Ff2wL': 480, 'GzZPVLUSXpuWxqPxJ5': 481, 'slwAEhc3F08': 482, 'JQCkCRjfu15Uujszrk': 483, 'nMOXstSwwBLjpa3': 484, 'L3SRV': 485, 'lwJVtMA': 486, 'xiIrw3w': 487, '2yPDH6': 488, 'cEyi1OGa6c': 489, 'bt7N1s54oqE4UDhwcv': 490, 'HLX6teC': 491, 'LEMtb': 492, '1qeBoYKlViv': 493, 'SjZHNDfhkdcSUBsyVM0D': 494, 'BnjmMmhmFnXYb': 495, 'Lz94SmuAbvXcofyD': 496, 'CVLOcfBqr80Ga4': 497, '0OgUul0TnmRKxcWjP': 498, 'sJO2Pl5': 499, 'sQnjEKeOTPOWi2KxA': 500, 'dmGhn': 501, 'l0nyjD4Q': 502, 'eOZXeDVVtBSV6txh455D': 503, '6MU3h9IWk0qzS3BwmgO': 504, 'JFSCd4qam6V9cvvcdym': 505, 'QlmTocJV': 506, 'oJ3QJOg': 507, 'iEeAaK5iVP928d': 508, 'NpsAzBTG8Be': 509, 'Bert6I8DM03VRif8vS2w': 510, 'yidDlnhaPWsjZ7lpR': 511, '0j4SK1dRE': 512, 'qtVZ66HxHNfg79rJdoy': 513, 'KqVra6gCB': 514, 'uXkwb77Qekn7C87412hE': 515, 'nBdWOtzld6MM': 516, 'gmEw44w7jGlVN': 517, 'HZVVqupxseBkMUj': 518, 'IQYJVN045g2SeIio7': 519, 'CBQ6AtnKBGH79TSYN4': 520, 'oxVJ8HQLQT': 521, 'XihuIA': 522, '07EYq7AP36T21eUJlpO': 523, 'ncFjbICf1M8ake4ua': 524, 'HQkeF9pe': 525, 'fVVcP': 526, 'T3dkkOj784HxeX3zI9': 527, 'EiwIV0Qq7hYEpzdyC': 528, 'jM3aSSW6': 529, '7qXmpxnsZiN0tw': 530, 'kvSFOLID74VpXcVtcM': 531, 'reXxX2yDe61H': 532, 'EBgu7Ky': 533, 'TTmKlHf6xdp5prkrgiUK': 534, 'vJ4veE': 535, 'QGPVr0cWbC': 536, 'pzZeuVjf9lAPfKkmZOq': 537, '1vbhDSbsAF': 538, 'GOIdc1lYtH3b3hb2lp': 539, 'Ul4Zzcu0WXnt4HgoMC': 540, '80OTb4XzqqmJQWIdTJlV': 541, '6Y8NHVTE4ddJfxoXT8U': 542, 'nFCbotoaaYqhSm': 543, 'ciow895AVHcJc2nv2SZb': 544, 'aDweh': 545, 'dFJ0iywon2': 546, 'Z8jN1dKZ': 547, 'kjH4CEYsv': 548, 'sbSnyf44Gv5ieNOQ': 549, 'wO8kv': 550, 'kgqfU4nG': 551, 'sx1wUQdYJgk6AV6X': 552, 'npqbFJ9zw2Kg': 553, '1ZKhao1GRS8oPne9v': 554, 'Gz65aktHS3pemba': 555, '21tU1Q': 556, '1LKcOgyQmuRencaPo3': 557, 'cIEu7GO': 558, '0BWOTkMZsv8oAy80': 559, 'vxgDrkz1Xh9JB60eW': 560, '8wcBMC9DHaoY9hWj': 561, 'Qog2mk': 562, '6pDG6': 563, 'hdVqL3Hp7ahge': 564, 'sDtZwYo7WgmwcTHJxd': 565, 'mV26fBJ2nqQ89Q9h0yqH': 566, 'GH4yNwBQxeLCLjYrSg8f': 567, '4m9GAj9JhAD5Pyi7Pxy9': 568, 'ZEG9Jg': 569, 'Ha8uN': 570, 'YrR9wsS': 571, 'bshNAfBHr': 572, 'R8i4WTxItQz2': 573, 'TFzsc9HtAseXVY': 574, 'Oh3lxfehnXJaEzdbW': 575, 'raqBSzVF2': 576, 'tgTOlu6dW': 577, 'RlgB0Rhsq8xQjy': 578, 'FAS892KewxlQqyC': 579, 'i7GYaAXcz7MK': 580, 'vPLST': 581, '3xVUVYyl': 582, 'Sackhtvl6is8v': 583, '1P9vitM61d3zihs': 584, '5dQbo': 585, 'Qb1aM5uu7MZaXKjSylF9': 586, '7Tc5V': 587, 'pTYaVb7oOBpclKNwQm1W': 588, 'IvIf9hR9APELjUnzX': 589, '3TSvMhgSfYPqRy7Q3': 590, 'W92NMCPRUaBzooPENNWD': 591, '6OngtAPhV': 592, 'VHO4mO5tcKuU4N': 593, '5uEgxqaIvWpFFu1LUV': 594, 'cT3BZi0d': 595, 'VEZcv': 596, 'pIOWjq2s5': 597, 'zynWg40UCWT': 598, 'ikwvidL9vHuHDDkbAZT': 599, 'PpLmFFoO2t77Z8JHC': 600, 'jARtCSACUI7d5k': 601, 'ksT0EjSKRE': 602, 'tkDJj3RadYF3fZ5l': 603, 'ihCWEMRAsA6IpYWPp': 604, 'iMlXTYgbUaeZYjk0ms1V': 605, 'FgYSXCxvyx': 606, 'uKlKw': 607, '8nND3VEr0v': 608, 'xtTxg0Esb7AOE7IEzp': 609, 'r1Kqc0Gp3XBcXGqk': 610, 'D6SUSiwJqGNc7Q': 611, 'skogcP58Wo3f3L': 612, 'a8UAoKFrafF': 613, 'UKUpMRZWnIBlxkVcHI': 614, 'SUuHyjfcfwpjvapKFqi3': 615, 'fzafn': 616, 's3HGWP2': 617, 'XEq2p': 618, 'RVN0FrblkHb0ORJI': 619, 'xqnlTZio3FopO3uvq': 620, 'PgpEpeK7Q3gBa5C': 621, 'bhtJEOxXFPdAF6lyyGdU': 622, '5cozmw7poDqOSb3': 623, 'Tqjv6aF': 624, 'vxWHdnx5icDr': 625, 'FZrd6onpvsdsm': 626, 'fLyJr3EN4QIh5TJRnEt': 627, 'e23B4ToWf': 628, 'SxX6RV': 629, 'ZEPo1Paepowo': 630, 'Nxg5FpgzvxvSGU9L0y': 631, 'A0L8nufWaAKHEPYAt': 632, 'fh53RkU7': 633, 'CYNvkOuHCvsyRDHlGy': 634, 'UlIMzSvNsGWI3I': 635, 'YIGVN6': 636, 'gty0ZWbrF2dwXtM42': 637, 'vHp8KmckeTkLwZX6et80': 638, 'mizCPg': 639, 'r5SsXwUpnx7': 640, 'zg1XAbf5qa6VwZmHy': 641, 'OhO8iC3sxow1T7xXX': 642, 'fagOkqrhidE3suo35Hv': 643, 'SGrIu0XZL2K': 644, 'oluLctNYRaoVop328x0': 645, 'B1cN7i': 646, 'ibhY2L2VRfmM1JayBp': 647, 'zDP6ar7': 648, '5JpDOdG1Wla': 649, 'ojG1i4wWWrhsafer': 650, 'cW78C': 651, 'FJyxPBIOcwpPEFogl': 652, '1pD4IyZPE2Cgwk2dr0q': 653, 'AXLcFVKCgcpMlFBij4ah': 654, 'hYrCUmH': 655, 'yJEbMP02VvTH': 656, 'Z0eIUWV61U78': 657, '59h9gsCkEXocRb7TiI': 658, 'G9DlZPIzt': 659, '13Pi0i58gmxAulaVq': 660, 'cEJuX9sOufV5XHqro4o': 661, 'SubLFcKhrZDEzcAN6D': 662, 'axYOlxAZJgNXWb6SSQ': 663, 'UB3XQ55': 664, 'IVtv02iXhsk': 665, '4J7w9gnDAUieDE37': 666, '6nfmv9Yzn': 667, '5REkXYK3q': 668, '1wknieaE1cinGck6': 669, 'n3ViqlCE4': 670, 'VBMnVaZjPttNQ': 671, 'LUDDk0': 672, 'tDdZNEG59GpGJQqXI3': 673, 'mKsiprD5': 674, 'j90C9FaYV': 675, 'zerDrBmq22e7ZJg': 676, 'fJCmIaE5z6pID72I': 677, 'U432UCbZtrz06o25W2j': 678, '2w60xno3ufk0c08hWFE': 679, 'nxBrSat8UuZ': 680, '8QycV6': 681, 'KlfKPnAWGFMJMv': 682, 'dJr9H3bruYhMagDgU8H3': 683, 'QruJoVdw2yb2': 684, 'bNE0xFFoMX3tx3Omc6': 685, 'vc9YbGMoVGltkX': 686, '5Z6xcPI': 687, 'OxnuaFwj0B': 688, 'gz82YMDnW8HJft': 689, 'PWjZqDS2': 690, 'ZAVvPM': 691, 'y99uJIxqHdYZe5KL': 692, 'v0TRdKe06IobiHA': 693, '57nJA0R5': 694, 's37SfuqCZ2NcpIYzpL': 695, 'vTByD': 696, '9Cg5AS': 697, 'XizBJG7kzLQN3stCXv6': 698, 'EUPHC0FLTSh': 699, '0ccCmMl6nsez': 700, 'REUnE7': 701, 'MpKk3phZNmv5': 702, 'LsYWVDG': 703, 'SCqNiSP38M': 704, 'GknUn4g': 705, '2BGm6tVEuiqOs9xv8': 706, 'zG5hA7': 707, 'tDgMDDaVqk2e': 708, 'wJwiV': 709, 'e5IuMPGUFmCJP8': 710, 'KImXk': 711, 'jOLcY': 712, 'XA2IUmRKOr7PY5q': 713, 'U5kDIh21Sx6JKqGw': 714, 'jHzzZb': 715, 'fiXtcnyu': 716, 'tCrV3crkzgd': 717, 'U2980Gqc': 718, 'av6GPu3CsFKo': 719, '8g5IeHCgRWO6pf': 720, 'Eo6G55qBXI': 721, '6V2y3ElZXIkZvdT5T': 722, 'wm5LzaRuul3fo2bZMA3': 723, '1LZxreqq6WsvS': 724, 'kdSYTaiBki': 725, 'B1bRFFvU3xBoqNZbdc': 726, 'TUoe2zXx3JO0Lc': 727, 'CIUGwF': 728, 'chb22RGXu0UjybQ6mko': 729, 'h5yKVfZdSoRCW8drAeSq': 730, 'qsF3YcX': 731, 'QJhKgf2P6L2WbJlV': 732, 'nHhNmb': 733, 'thWA2HdWnny4TcWE260': 734, 'Yoytlu1B': 735, '2oVpxONGoeR37OWGjOQn': 736, 'JKIBFjiy9VY': 737, 'hQbbLJI2': 738, 'mbY8w6Dt': 739, 'VahQmA1r9': 740, 'z7QS0': 741, 'jyjMXa': 742, 'TEl0wMMjVR9': 743, 'l9DrjsOj': 744, 'jcuZ8SAY4u3Db12': 745, 'ZLMfso': 746, 'kvadqigZfZXzp': 747, 'jk7ztEy3uGn5Yj': 748, 'AxiyiT5SyVa4': 749, 'nzauL3TZiFBQJ': 750, 'sMcARwyVZVjAFr72OFz': 751, 'pWxKoPXQZxZ9wtf3a8': 752, 'MVPJ9I6Jg': 753, 'scbIhg': 754, 'Lysdlzj': 755, 'Xm8L6GYAuCxIWYmc': 756, 'hOv7APPtZ8SJectVFnu': 757, '0kw0FkA8P9O': 758, '4UXhC7cBPJAOr': 759, 'qKc7F3nSWo': 760, 'SDvSntDFyPgBphS9Nw': 761, 'K4zLcl6cvL03lDsO2l': 762, 'pBTAH76kF6': 763, 'DThB2sQrD': 764, 'glooZ4K': 765, 'RNGsjCskWVw6t': 766, 'RlM3ZCOf2': 767, 'Im06eo': 768, 'ONkvPnbQz2zpa': 769, 'l8DLoH': 770, 'SmCvkYJDnRiOwV': 771, '7RpyBLORuf': 772, 'dd1fWa6YVF0ZeSHs': 773, 'G9ZMisil3R': 774, 'Qz0qPsL0TJHB33wo': 775, 'vZK6tE': 776, 'vJ8yNQiL': 777, 'YvWSWIU': 778, 'APRIV0vdn': 779, 'xCXlIWbMD': 780, 'AgZdMIo58iV': 781, '6VGpAm3bdLX': 782, 'sAOQFHo': 783, 'T6M7Z8H1r3pF8I5HYD': 784, 'C4pxchHM5jONaPScwSL': 785, 'WwQZAIBtX3mEyl9': 786, 'gLYvHGk3': 787, 'DZf3hr6WHyuDZ4': 788, 'FKiIoH9OGLkPrZn': 789, 'dTc23mouIbRiA1o78HM': 790, 'mF2B1': 791, '7YZ6zmvf8ReUAE': 792, 'HJalnJA8': 793, 'xxQd8ZYc90y7B': 794, 'jcnTb8gziWM': 795, '2AhuiVNWqiNt': 796, 'KGD4Xrnu6a6nNOJ': 797, 'Hh4uOdgmY3LxVm': 798, 'EBAkJJ0mHe63y9m2A4K': 799, 'Wocuy4': 800, '9KpI5wbL0mKDni': 801, 'hQWsFUwKB6aojDv': 802, 'Tu5huOMPKTUNhy': 803, 'dRs6qIkulR': 804, 'zwoKHV': 805, 'vsC9Ii': 806, 'MIDsaOaRPu5': 807, 'bZsjz': 808, 'Kg2y9xqMH': 809, 'lKu9O0sOpc': 810, 'IG8e7S1zusp5AIm9FF': 811, 'F6qVHakrDWHa0b90': 812, 'Fc0xlVGV15qQi9B5tIe': 813, 'KqU0bW0cvjs': 814, 'zLF0xuSa': 815, 'RHbSWg': 816, 'Fxr83wsxYWv6IHc5': 817, 'VUUHSN': 818, '0eME5kEuJW': 819, 'fv4uuo8fGYRAsz3': 820, '7iuVI2': 821, 'pXTlQ11jb9ynDW': 822, 'FWiZFT0L': 823, 'pf4Ct29FQ63pEi3': 824, 'gk7oKttq': 825, 'rAV5pyyxO': 826, 'vd2yiD8': 827, 'htdzKbRBdGRh46uR': 828, 'oO1HJ08ll': 829, '2Ze4FjOW8vQVfI': 830, '8NTj7qHElVrn7V': 831, 'bdB7YqmX': 832, 'YKDdJmKB': 833, 'vq3nXOC3XJ8HWBujO2R': 834, 'FRsKiCOaNAII2': 835, 'IMm911bqXJIiKdyrPx0g': 836, 'o2WoeQ0qLeIlK': 837, 'K8nOtNQGxhFDp': 838, 'ejalUdi': 839, '3HwWLF0m': 840, '8v2Br2Yx4x': 841, 'KeoLGZ3INIpi': 842, 'BMFsaAcwV6xaVPAla': 843, 'nYqPM8M': 844, 'uNYuF': 845, 'sA8eK5NZV': 846, 'dOrpZDOMQ7hf5f': 847, 'dcNkb1': 848, 'S94xl': 849, 'oPZnJBtaWqen8IIUw': 850, 'BYHEg4ovSmBZ1fcPT': 851, 'DTgip5HziiP242FF8': 852, 'tdZhp0URuUwSP8BRsFF': 853, 'pwbP7kc8eTn6VnKet': 854, 'wbBbuUGiRfMBtc': 855, 'NhwJFGM': 856, 'aGll2DHQsXxaGXN': 857, 'exMfyGtP9BFVO1': 858, 'TZq6nKqupyRoDEjpo': 859, '9j55vHfqTdpNBOUJ': 860, 'OjPlirMRgClfmd8V70rF': 861, 'kif0k4729H3': 862, 'QtAcBdYaOtKrR': 863, 'j0dCpWCKSMIQUK1vCcpy': 864, '9HgSfr7jZyfkLkqD': 865, 'uU6YAcG': 866, 'bPoiHsFnUHgL': 867, 'RukEAzQCou5': 868, 'jj6MaBXPll06lT7UTlHF': 869, 'MTyNKT0k': 870, '4Bq5uhWHJmc70p': 871, 'NnSvJqWHhzFfsi': 872, 'oFphOSxHWMNy6oWDv': 873, 'GNtk06X': 874, '23p0tnTzp': 875, 'NJEYB': 876, 'C4MkyTqz6vh0e1t': 877, '398J3V8HowTErgjasxcU': 878, 'gbFfvkDsitgOReNSKk': 879, 'cL7gIx': 880, 'DL0CYoNtbMPFsw': 881, 'QS4sPqLDrkhM': 882, 'MZAI0t6kMNkJ': 883, 's1Dht0fyo': 884, 'ASdKo': 885, 'boqyxITZ6r6d97shmCD': 886, 'sZxhLgjlo': 887, 'Uihd1YiIuglfwiNdkG': 888, 'WwAjFlw81ml': 889, 'AMeGO': 890, 'LN9WcM50m': 891, 'cCmAXj4ZmhRjq7sie': 892, 'TTgkIl2z7nqXr553': 893, 'e5x8GGDY4gctgBVGPK': 894, 'K6XG56r': 895, 'a5uZ416Zc': 896, '4X0cxe1pxzCvjaiOAG': 897, 'J7R1ElhwiXCN99YX': 898, 'oFqqY7D': 899, 'FqHNTnimFDgNGEtF50c': 900, 'sGSPCuJw3ubGs': 901, 'rnoX3BM5mvK': 902, 'MIK37l49rkGovdcpK': 903, 'E9ucNg7QElmCEbCD': 904, '6CRTK': 905, '6nNwBGDNAOjHxbYoSU6T': 906, 'B9X3GpHmqN054': 907, 'KOb94RNRM3Z4RFPh': 908, 'YlgzL7etdemkT': 909, 'YQaJF7HRpQy9tbCMY': 910, 'Dd3VeQ09871C0': 911, 'WVZGLgYxph8': 912, 'QrITavS4YJHkjZIIppaZ': 913, 'xAJqlwOJ': 914, 'cQcuovQaB4e': 915, 'Uebzl2ry': 916, 'NP7qzoDn8letWA': 917, '1xgf2x771': 918, 'N0ndm1': 919, '79yLUDHcLI': 920, 'K0RStNJGedm2aALF': 921, 'C9Nf1z8rKlRcZNnFd': 922, '3JR1K': 923, 'vSurtvNZGmEph': 924, 'ywlJObPGfH': 925, '6ICnKbiPuGt3r': 926, 'WXugnkPIfa': 927, 'ybY9V2TE86qWsIu2d3Z': 928, '0joKfcA7Y': 929, 'hAfnj7b': 930, 'qfgmtdd': 931, 'W0jx1bxw9rLaM': 932, '5PWtxxcVPF91TFa': 933, '8Cx2IRb9PrsfU3rN1K59': 934, 'FLjntoFYqJh': 935, 'CntpjaCAUbLbD6MiiBVI': 936, 'J2573': 937, 'HcVavmd2MQ9': 938, 'oO0SumYvgwSw': 939, 'Vetx2fKH4uiqNShhiG': 940, 'Wp08HjzxDN6L9p4PQEq': 941, 'GfAM0idihsFPjr': 942, 'FcncV3FFP': 943, 'Nfb2eiQCNDPVWrbklBXA': 944, 'q3Ghw2Ohhf9ZAAZ': 945, 'CRG1d0zEL3': 946, '77HqxFVMZlutJwn': 947, 'F4N15': 948, 'xcy2XQeukiGHh': 949, 'oBNckXxmcC2': 950, 'PFH4KUwECAYio': 951, 'ET9OlAbUG7njv5OooUSE': 952, 'aqBLxLMkbd': 953, 'ObtGLLD3': 954, '0wOFObQBD': 955, 'GqgrjTOu65v': 956, 'QlFkl2ZHYiAfizU5jcoT': 957, 'WUYdoryJWLxmPAHnh': 958, '5mftoHqUq3n': 959, 'xn1Pf8': 960, 'JVrJY3PU6f': 961, 'Dy58qDUD0UtRp6B1Y': 962, '3l0w9xZ2qNf': 963, '2XMX8gWjtD': 964, 'o6onw1wjEudGQvUm': 965, 'aPhmtaj36GhKz1D3YI5': 966, 'b8odFFjru68TqU1hnCYS': 967, 'Gwu6mEuvf0EEKUBvY': 968, 'mVmWyYoUZOsSmA': 969, 'yPNpZXM': 970, 'LZmZ4VKH': 971, 'Z1G7W5akWifk4L': 972, '3xuBH6p': 973, 'miufwJR': 974, 'zxnyxFfpLisZMQDls': 975, '3dEAbke612F2t': 976, '28RRuSqpo3cKq2j5k': 977, 'YcApU': 978, 'O6MUVzDfoXp': 979, 'zVWo2afw77PbcV3bjqsk': 980, 'sVqHXvZSykAMutjIxE6t': 981, 'd3o42hJrmewG': 982, '7cOYvwwkdbqLRXfsYik': 983, 'QlBMxPdMgEYOuOkMRn': 984, 'yTsKzymRAFKs': 985, 'MCYWCNEsZigMX4': 986, '3ricEPIeYhR5Z': 987, '85IGkRG': 988, 'Bmbdk0dlWFDBJFE5WjIp': 989, '9Lfb1CdUy': 990, 'rAXQmYfKV': 991, 'QzwYcAXuOAbi38': 992, 'QVowSlObRn': 993, 'NUj0iAoo': 994, 'uttneHur7icKAlC9erg': 995, 'QzmcWuCblYqBl4SoD': 996, 'TDH0hY1xW8MMISBbXjO': 997, 'WVaPQOOFSH': 998, 'kwuN0udbyM': 999, 'Cm1ozvQNl2KS': 1000, '2ZaJ3ZA': 1001, 'ZiTVl4uGvmuagj1uzLxr': 1002, 'emd15jrrJY': 1003, 'inLi13Nhj': 1004, 'gIeIMaEQx': 1005, 'HWeMVf8JBUrtlsU': 1006, 'nNuXZ0fLoSUpDp': 1007, 'aQvEpfzqB2AN': 1008, 'rPbZ2h9U12w': 1009, 'lu6hsRn': 1010, 'NwenVoaDz': 1011, 'KzgAo': 1012, '3FLUBUjz': 1013, 'NyNezWBmmVTxya03V': 1014, 'qBsHEWt9DZLOi': 1015, '2D4EiV7': 1016, 'NOvm4vGs34dY1avQp5': 1017, 'a7di9liNeV5UeLR8Lk': 1018, 'FLop5FID3Xkc': 1019, 'uFnYgJtpP3jZwbrZ': 1020, 'KbqHsNa': 1021, 'hdLkpWPZJMepkwRh7z': 1022, 'Yx9rEzD59UjTAYrTrB': 1023, '7NOVQUWG0': 1024, 'oDXkWymAF6764gbGuQBi': 1025, 'kqVMXgElZSn8': 1026, 'QKgr9r0Qn': 1027, 'iBSGazhFqHEoI': 1028, 'f1neXgasIbLzRt': 1029, 'tLRqT1k': 1030, 'BaYhQMXVYCqk3e6He': 1031, 'RrBjOJ4sxfnRxxzG': 1032, 'wICGlmpBQKkLSFGYnzBj': 1033, '4P7Kcr': 1034, '2ew74ETm': 1035, 'BbnPzNzfnf': 1036, 'cRk5czfy0rk': 1037, 'PqDbnAb76LcO8LF1B': 1038, 'crkBsj': 1039, 'biSaNgG': 1040, 'g64b5C1ZjWvP': 1041, 'Lcepqh0': 1042, 'pQWOQcPiWjWHQBF4Bca': 1043, 'hFI8wssZ438diG': 1044, 'jot6KliKOq': 1045, '4RDostSQPmOQWx': 1046, '7Hfe78dhw6Nww5OZL': 1047, 'cKUK6': 1048, 'UXPd7': 1049, '0xQauIwJKN4PAIh': 1050, 'ICvAq2RfhIrqO3FOFxc': 1051, 'oVMLXt6kEF': 1052, 'mlmeYWipD5': 1053, 'djsuXFPoIAOwYtn': 1054, 'lftzZ': 1055, 'Jx4r7Qjgm0d0XFNvm': 1056, 'fOvr2bVHsRocm1': 1057, 'WPJnkQcHZD6rI4yQH': 1058, '4cOLyAAx930fQ31wEc1': 1059, 'GOLt2bmv3lG': 1060, 'SD6FW4619TmuZHoZGmX': 1061, 'CgqJrRTx9gnSFONVU0': 1062, 'CEcKxPWCCRAhf': 1063, '4GMSn': 1064, 'BvWjan': 1065, 'HqoE8Eg9Uh5': 1066, '4mtRPY0LcY1Oclq': 1067, 'IWfCHf': 1068, '5gi93BcEwwCADy': 1069, 'F2DFmGoptIPo': 1070, 'WKNqXGBLuFooQ4rt': 1071, 'iM25wGACiBh7dk6XCkTJ': 1072, 'YDkMNx9tCQf': 1073, 'ZieYnAR': 1074, 'ZKOQunI8AJy': 1075, '4Y2n5S9u7liJ': 1076, 'T8LF7S': 1077, 'dbona': 1078, 'KlkJ7xhTAtH7Ixv': 1079, 'DZmXAsYlDs4ze865m': 1080, 'tYjVTZWzpBG': 1081, 'MlrRC4cXT': 1082, 'PJY5ytfYsNOG3aa': 1083, 'xLJwXAx1bwnZvbS2Ia1k': 1084, 'TqITd4oh': 1085, 'WBE1RDF': 1086, 'Ubsc1T2nLI5zMk': 1087, 'WTQ1mgGYHlmyIH3ZPYSX': 1088, '2DImopSukjQNbhqoh': 1089, 'pciP8zA3yBEmxXM': 1090, 'jQlPrzuwd6XsvFUsF': 1091, '88Jk6YC': 1092, 'VNx5I0iPm9QNYhICg': 1093, 'leHfGzgMuRAqhVbz': 1094, 'kqzx1A17wREX': 1095, 'nyhUzak01': 1096, 'S396m4MyS0z2m': 1097, 'ZW8504dI9Z': 1098, 'Z4X32': 1099, 'YPwNblr9P48NoJI': 1100, 'yrPS1Kim2QzAkHp': 1101, '4CP0lgP0AT0hN4t96Cys': 1102, 'TsALVqJTxcQc20MGqc': 1103, 'dMfvOQVNvBXOpmEnvR': 1104, 'K1KQngRMxu': 1105, 'TstwrSA4fRtMgk': 1106, 'HYjJT1axGkMgc9KdzI': 1107, '3vxdvhlDKOtSLZQiOkXm': 1108, 'UJzpCWrisjWSkMqTcw': 1109, 'JEYWxA5qB': 1110, '1Jjh3b9be0jLuzAsWUAf': 1111, 'bTcs16baOcJ5PXI9O9': 1112, 'TzYq2xpqMRDVib0m': 1113, 'x76t8yo8': 1114, 'Szk4w7g': 1115, 'YBdQVys3h': 1116, 'oOTSoX95JonOCEm9': 1117, 'KfQnXDERP': 1118, 'gUzSxNQAkvGXZ': 1119, 'umK0eidE': 1120, 'NX4tFHFW': 1121, 'kWGmu': 1122, 'taDe2AKvJYD6lVM': 1123, 'fOr5BbUVzrmNa': 1124, 'hpjmT65YGjv': 1125, 'X40pnSr4utI6cmKcGUhO': 1126, 'S3cGUgNaC1ut': 1127, 'BoEYgiOl9': 1128, 'ldtmagpZDgqnIO9j': 1129, 'baom3KZg7oyiPOL1lV5': 1130, '3HCxi': 1131, 'cv6ocF3bNiprry3Bl': 1132, 'SsFl0wh4UjXd': 1133, 'PcFH2iwZ0': 1134, 'v8ZeQM': 1135, 'SkUQbjfwpqvMnlPpd': 1136, '3abuYleIjZ3D': 1137, 'ZIRjAZtdTdNmLHq': 1138, 'cKNIiLY': 1139, 'ISdK4s': 1140, 'zy1IEzdSLHp': 1141, 'JOqOHwY': 1142, 'D6Y0TdDwsh0dl2mps': 1143, 'kQlES': 1144, 'znJq3UPCXt7L7OUK': 1145, 'ZJQnH48o': 1146, 'h1PH1BZXGwW12C2A': 1147, 'n7Vkf3': 1148, 'y2AmCJvX': 1149, 'y4uwyxO6x2s6MgdQqBwz': 1150, 'q7Z2ZmALQJY0lVOD98fk': 1151, 'cauCz3q1D': 1152, 'R9Rkgr9kukEgM7IcVzoV': 1153, 'YPmzwipD4s': 1154, '90fCuG6NAOzFHyNoW': 1155, 'vuZ4QT0mf1gt': 1156, 'SLQfZkI4': 1157, 'rpTHSqos0JeTeiYMwKHz': 1158, 'j84wgOJbYIY45pL1xEs': 1159, 'Ih6ZhShRp4BUlC109Wkk': 1160, 'qcxnmlQa': 1161, 'zpVgcrEtbFiM1u': 1162, 'rKfkm9Tr': 1163, 'JdOaTp2iSmjTteo': 1164, 'DoFMOsH7QM': 1165, '6dYXYA1gEZ8wVsUDMHM': 1166, 'vgZ48cDgqQGN': 1167, 'QMzKJ2': 1168, 'Lxw9MkdKipVt': 1169, 'KXbUs1hMenGijRm': 1170, 'eeF7ncEU0TaY': 1171, '5QZKnm': 1172, 'jPhO5x4qH7HKhL': 1173, 'AJLmQ': 1174, 'goGsZi882wIBZi5oeqCs': 1175, '6uTG0': 1176, 'K1hKb': 1177, 'vBYeoVqDDqjLKlx9': 1178, 'd7CO9Syz5lv': 1179, 'R6ACq': 1180, 'ixVRP2lPY': 1181, '3UjQtLx4Je': 1182, '8BxofRHdXqIC4hW': 1183, 'l9RhueFvnGSSnzO': 1184, '8YswR7rr': 1185, 'ioKgR9PgMZsM5JtUs8s': 1186, 'v8cvxGC2': 1187, '6clkzBnD9Qu': 1188, 'Mxohy6zq': 1189, '0JArSe08N': 1190, 'EAPIp0': 1191, 'jVC9sMVZst': 1192, 'vgkNB9e': 1193, 'Q6GXkZ': 1194, 'KJ7vtWons8m8Ycu': 1195, 'uEaLWGLicFxhyjJ9P': 1196, 'blThz8HIacCWS8oUezZz': 1197, 'It6DT4': 1198, '6BnMeUGzes9VJ6Sg5': 1199, '7PT3CHHWk3nHwHs': 1200, 'DelTHO6WC7EfH7nsm': 1201, 'OkKrjexuh': 1202, 'ijJ925dwHwAl7fVI': 1203, 'DEplsDtPVsg4XyxD': 1204, 'neyMBbV2i1HbYnt': 1205, 't80jyyF': 1206, '84HjV': 1207, 'zPMOc': 1208, '9A7v685oa': 1209, 'Vws3eL2y': 1210, '6PWRoBRrui': 1211, 'FySLPcjtp': 1212, 'Bq9A3EDZFK03': 1213, 'vpwRDH': 1214, 'ydqmId0cO1jRKlrVQha': 1215, 'VsL68B2McR2A': 1216, 'd6SW7BCG2LZ': 1217, 'pfJ8zulfxDG2R8i': 1218, 'koC43FTyec2': 1219, 'RglILAm': 1220, 'WrvNuRss1M': 1221, 'BtAJSaV7': 1222, 'zgabz': 1223, 'bKQuZM6jZnEsoFwj': 1224, 'pLSi71Gkabu': 1225, 'AIx75fW': 1226, 'j9rDVPqU1VlCzTdYUrLU': 1227, '5ts2hA0LJPD': 1228, '4B1EL': 1229, 'R3MG5c': 1230, '5wNoGySL2mseYD56tRDh': 1231, 'Z01co': 1232, 'BnfTBzuf86zSh65': 1233, 'tCSwrZBXMlN4': 1234, 'w2ZPyP3Bw8W': 1235}\n"
     ]
    }
   ],
   "source": [
    "def load_vocabulary():\n",
    "    vocab = get_json_from_file(os.path.join(generated_data_dir(), 'tokens'))\n",
    "    vocab = {key: index for index, key in enumerate(vocab)}\n",
    "    return vocab\n",
    "\n",
    "vocabulary = load_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-17T01:55:15.404089Z",
     "start_time": "2020-11-17T01:55:15.375012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_html_size: 3850, max_json_size: 1081\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-8ff658018d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                          prefetch_factor=2)\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-8ff658018d8c>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Convert tokens to numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mresult_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-8ff658018d8c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Convert tokens to numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mresult_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "dataset = TransformerDataset(type_='train')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    len_all_batch_html = np.array([len(batch[i][0]) for i in range(len(batch))])\n",
    "    len_all_batch_json = np.array([len(batch[i][1]) for i in range(len(batch))])\n",
    "    \n",
    "    max_html_size = len(batch[np.argmax(len_all_batch_html)][0])\n",
    "    \n",
    "    # We add 2 here to accomodate <sos> and <eos> which all target sequences need.\n",
    "    max_json_size = len(batch[np.argmax(len_all_batch_json)][1]) + 2\n",
    "    \n",
    "    print(f'max_html_size: {max_html_size}, max_json_size: {max_json_size}')\n",
    "    \n",
    "    def split_batch(batch_item):\n",
    "        for i in range(len(batch_item)):\n",
    "            yield (batch_item[i][0], batch_item[i][1])\n",
    "    \n",
    "    result_batch = []\n",
    "    for i, (html, json) in enumerate(split_batch(batch)):\n",
    "\n",
    "        html.extend(['<pad>'] * (max_html_size - len(html)))  # pad html\n",
    "        html = html[::-1]                      # reverse html\n",
    "\n",
    "        json.append('<eos>')      # append <eos> to json\n",
    "        json = json[::-1]\n",
    "        json.append('<sos>') # prepend <sos> to json\n",
    "        json = json[::-1]\n",
    "        json.extend(['<pad>'] * (max_json_size - len(json)))\n",
    "\n",
    "        # print(f'max_html_size: {max_html_size}\\nmax_json_size: {max_json_size}')\n",
    "        # print(f'len of html: {len(html)}\\nlen of json: {len(json)}')\n",
    "        # print(f'i: {i}\\n{html}\\n{json}\\n\\n')\n",
    "        \n",
    "        # Convert tokens to numbers\n",
    "        html = [vocabulary[token] for token in html]\n",
    "        json = [vocabulary[token] for token in json]\n",
    "        \n",
    "        result_batch.append((html.unsqueeze(1), json.unsqueeze(1)))\n",
    "        \n",
    "    return result_batch\n",
    "\n",
    "# Maybe worker_init_fn should be used only when we have GPU to process\n",
    "# multiple workers. But, shouldn't this work with the 4-cores on this laptop?\n",
    "#\n",
    "# print(list(torch.utils.data.DataLoader(dataset, num_workers=2, worker_init_fn=worker_init_fn)))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=4, \n",
    "                                         collate_fn=collate_fn,\n",
    "                                         # shuffle=True,  # Cannot set shuffle=True for Iterable dataset\n",
    "                                         # num_workers=2,   # (error is cannot pickle \n",
    "                                                            #  '_io.TextIOWrapper' object)\n",
    "                                         worker_init_fn=worker_init_fn,\n",
    "                                         prefetch_factor=2)\n",
    "\n",
    "list(dataloader)\n",
    "pass\n",
    "\n",
    "# All datasets are subclasses of torchtext.data.Dataset, \n",
    "# which inherits from torch.utils.data.Dataset \n",
    "# i.e, they have split and iters methods implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T01:22:04.976851Z",
     "start_time": "2020-11-13T01:22:04.972054Z"
    }
   },
   "outputs": [],
   "source": [
    "# This paragraph makes each of our runs deterministic.\n",
    "seed = 0\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T01:22:09.574254Z",
     "start_time": "2020-11-13T01:22:09.570571Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = dataset.split(split_ratio=[0.5, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T19:28:19.274452Z",
     "start_time": "2020-11-06T19:28:19.268121Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T19:28:57.681089Z",
     "start_time": "2020-11-06T19:28:57.675092Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T23:15:09.285236Z",
     "start_time": "2020-11-05T23:15:09.281597Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T21:29:00.074052Z",
     "start_time": "2020-10-24T21:28:57.917124Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/Seagate/generated-data/tokens', 'r') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "    vocab = list(map(tf.strings.split, vocab))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T21:29:00.087831Z",
     "start_time": "2020-10-24T21:29:00.076201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.lookup_ops.StaticVocabularyTable at 0x7fb6e6378c40>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we're specifying all the tokens, we don't really want any\n",
    "# OOV buckets, but StaticVocabularyTable requires num_oov > 0.\n",
    "# So we set it to 1, although it will never be used.\n",
    "def create_vocab_table(vocab, num_oov=1):\n",
    "    vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64)\n",
    "    vocab_values = tf.reshape(vocab_values, [vocab_values.shape[0], 1])\n",
    "    vocab = tf.convert_to_tensor(vocab)\n",
    "    vocab = tf.reshape(vocab, [vocab.shape[0], 1])\n",
    "    init = tf.lookup.KeyValueTensorInitializer(keys=vocab, values=vocab_values, \n",
    "                                               key_dtype=tf.string, value_dtype=tf.int64)\n",
    "    vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov, lookup_key_dtype=tf.string)\n",
    "    return vocab_table\n",
    "\n",
    "vocab_table = create_vocab_table(vocab)\n",
    "vocab_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T22:21:26.093902Z",
     "start_time": "2020-10-24T22:21:25.885101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before expanded dims html_data shape: (None,)\n",
      "expanded dims html_data shape: (1, None)\n",
      "test: 3359\n",
      "test2: Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    <ipython-input-209-95c50114d8e9>:63 get_dataset  *\n        paddings = tf.constant([[0, 0], [0, max_encoded_file_token_len-len(html_data[0])]])\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:263 constant  **\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:280 _constant_impl\n        tensor_util.make_tensor_proto(\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:456 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:333 _AssertCompatible\n        raise TypeError(\"Expected any non-tensor type, got a tensor instead.\")\n\n    TypeError: Expected any non-tensor type, got a tensor instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-95c50114d8e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# combined_fns = tf.data.Dataset.from_tensor_slices(list(zip(html_filenames, json_filenames)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mcombined_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-209-95c50114d8e9>\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mn_readers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                        \u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_readers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36minterleave\u001b[0;34m(self, map_func, cycle_length, block_length, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mInterleaveDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1832\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m       return ParallelInterleaveDataset(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, cycle_length, block_length)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4155\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m   4156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[0;32m-> 2938\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3363\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-209-95c50114d8e9>:63 get_dataset  *\n        paddings = tf.constant([[0, 0], [0, max_encoded_file_token_len-len(html_data[0])]])\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:263 constant  **\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:280 _constant_impl\n        tensor_util.make_tensor_proto(\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:456 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:333 _AssertCompatible\n        raise TypeError(\"Expected any non-tensor type, got a tensor instead.\")\n\n    TypeError: Expected any non-tensor type, got a tensor instead.\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data:\n",
    "#   - During training:\n",
    "#     - We're planning on using 10K generated files.\n",
    "#       Average file size around 9K\n",
    "#       90M X 4 (for uint32 numbers) = 360MB full HTML training data.\n",
    "#       Also some more memory needed to hold JSON training data.\n",
    "#       We can decrease from 10K files to 5K generated files, \n",
    "#       or increase the memory reserved for this application to\n",
    "#       hold this entire data in memory.\n",
    "#     - So we can shuffle this data as a part of the model.\n",
    "#       It is good to shuffle at least per epoch so the model\n",
    "#       is not biased.\n",
    "#     - You can specify:\n",
    "#       dataset = dataset.shuffle(buffer_size=100,    # prefilled buffer to speed up shuffling\n",
    "#                                 random_seed = 10,   # random seed set to ensure repeatability\n",
    "#                                 reshuffle_each_iteration=True)  # True by default. Set to False for debugging.\n",
    "#   - During validation/testing:\n",
    "#     - No need to hold the entire dataset in memory to do this since\n",
    "#       we can apply the model for validation testing on each file.\n",
    "\n",
    "import os\n",
    "from glob import iglob\n",
    "from utils.file import read_file\n",
    "\n",
    "html_filenames = list(iglob('/Volumes/Seagate/generated-data/html/tokenized/*.tokenized'))\n",
    "json_filenames = list(iglob('/Volumes/Seagate/generated-data/expected_json/tokenized/*.tokenized'))\n",
    "\n",
    "batch_size = 32\n",
    "num_prefetch = 1\n",
    "def get_datasets():\n",
    "    #     base_dir = '/Volumes/Seagate/generated-data/')\n",
    "    #     relative_html_dir = 'html/tokenized'\n",
    "    #     relative_json_dir = 'expected_json/tokenized'\n",
    "    #     def input_filenames():\n",
    "    #         for html_fn in iglob(os.path.join(base_dir,\n",
    "    #                                           relative_html_dir,\n",
    "    #                                           '*.tokenized')):\n",
    "    #             json_fn = os.path.join(base_dir,\n",
    "    #                                    relative_json_dir,\n",
    "    #                                    html_fn.split(os.sep)[-1].split('.')[0])\n",
    "    #             yield (html_fn, json_fn)\n",
    "    \n",
    "    def gen():\n",
    "        for (html_fn, json_fn) in zip(html_filenames, json_filenames):\n",
    "            yield (html_fn, json_fn)\n",
    "\n",
    "    def get_dataset(html_fn, json_fn):\n",
    "        html_string_tensor = tf.io.read_file(html_fn)\n",
    "        \n",
    "        json_string_tensor = tf.io.read_file(json_fn)\n",
    "        json_string_tensor = tf.strings.format('<sos> {} <eos>', json_string_tensor)\n",
    "        # When you format a string tensor, the string is shown\n",
    "        # with quotes around it. Remove those quotes.\n",
    "        json_string_tensor = tf.strings.regex_replace(json_string_tensor, '\\\"', '')\n",
    "        \n",
    "        html_data = tf.strings.split(html_string_tensor)\n",
    "        print(f'before expanded dims html_data shape: {html_data.shape}')\n",
    "        html_data = tf.expand_dims(html_data, axis=0)\n",
    "        print(f'expanded dims html_data shape: {html_data.shape}')\n",
    "        print(f'test: {max_encoded_file_token_len}')\n",
    "        print(f'test2: {tf.shape(html_data)}')\n",
    "        paddings = tf.constant([[0, 0], [0, max_encoded_file_token_len-len(html_data[0])]])\n",
    "        html_data = tf.pad(html_data, paddings, 'CONSTANT')\n",
    "        print(f'after padding html_data shape: {html_data.shape}')\n",
    "        \n",
    "        json_data = tf.strings.split(json_string_tensor)\n",
    "        json_data = tf.expand_dims(json_data, axis=0)\n",
    "\n",
    "        # Cannot concatenate along rows since the columns are different sizes.\n",
    "        # combined_data = tf.concat([html_data, json_data], 0)\n",
    "        # combined_data = tf.data.Dataset.from_tensors(combined_data)\n",
    "        \n",
    "        \n",
    "        # print(f'combined_data: {combined_data}')\n",
    "        # return combined_data\n",
    "        return json_data\n",
    "\n",
    "    def reverse(padded):\n",
    "        html_padded, json_padded = padded\n",
    "        return (tf.reverse(html_padded, axis=[1]),\n",
    "                tf.reverse(json_padded, axis=[1]))\n",
    "    \n",
    "    n_readers = 5\n",
    "    dataset = \\\n",
    "        tf.data.Dataset.from_generator(gen, (tf.string, tf.string)) \\\n",
    "                       .interleave(get_dataset, cycle_length=n_readers)\n",
    "\n",
    "    #     dataset = \\\n",
    "    #         tf.data.Dataset.interleave(lambda x: tf.data.TextLineDataset(x)\n",
    "    #                                                .map(get_dataset, num_parallel_calls=1), \n",
    "    #                                    cycle_length=n_readers)\n",
    "    #     dataset = dataset.interleave(filenames\n",
    "    #                              .map(add_sos_eos_tokens) \\\n",
    "    #                              .map(to_int) \\\n",
    "    #                              .map(pad) \\\n",
    "    #                              .map(reverse) \\\n",
    "    #                              .batch(batch_size) \\\n",
    "    #                              .prefetch(num_prefetch)\n",
    "\n",
    "    for x in dataset:\n",
    "       print(x)\n",
    "       break\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ds1 = tf.data.Dataset.from_tensor_slices(list(html_filenames)) \\\n",
    "#         .interleave(test_lambda,\n",
    "#                    cycle_length=4, block_length=16)\n",
    "\n",
    "# combined_fns = tf.data.Dataset.from_tensor_slices(list(zip(html_filenames, json_filenames)))\n",
    "combined_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T22:19:56.664868Z",
     "start_time": "2020-10-24T22:19:56.656132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3) [[b'this' b'is' b'something']]\n",
      "len: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 10]),\n",
       " array([[b'this', b'is', b'something', b'', b'', b'', b'', b'', b'', b'']],\n",
       "       dtype=object),\n",
       " 10)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "# print(t.shape)\n",
    "# paddings = tf.constant([[1, 1,], [1, 2]])\n",
    "# # 'constant_values' is 0.\n",
    "# # rank of 't' is 2.\n",
    "# tf.pad(t, paddings, \"CONSTANT\")\n",
    "\n",
    "x_html_data = tf.constant(['this', 'is', 'something'])\n",
    "x_html_data = tf.expand_dims(x_html_data, axis=0)\n",
    "print(x_html_data.shape, x_html_data.numpy())\n",
    "print(f'len: {len(x_html_data[0])}')\n",
    "paddings = tf.constant([[0, 0], [0, 10-len(x_html_data[0])]])\n",
    "x_html_data = tf.pad(x_html_data, paddings, 'CONSTANT')\n",
    "x_html_data.shape, x_html_data.numpy(), tf.shape(x_html_data).numpy()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T23:23:35.951410Z",
     "start_time": "2020-10-23T23:23:35.941835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'<sos> json_string <eos>'>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_string = tf.constant('html_string', dtype=tf.string)\n",
    "json_string = tf.constant('json_string', dtype=tf.string)\n",
    "j_str = tf.strings.format('<sos> {} <eos>', json_string)\n",
    "\n",
    "# When you format a string tensor, the string is shown\n",
    "# with quotes around it. Remove those quotes.\n",
    "tf.strings.regex_replace(j_str, '\\\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:16:09.898230Z",
     "start_time": "2020-10-22T21:16:09.886948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'/Volumes/Seagate/generated-data/html/tokenized/0.tokenized',\n",
       "  b'/Volumes/Seagate/generated-data/expected_json/tokenized/0.tokenized'),\n",
       " (b'/Volumes/Seagate/generated-data/html/tokenized/1.tokenized',\n",
       "  b'/Volumes/Seagate/generated-data/expected_json/tokenized/1.tokenized'),\n",
       " (b'/Volumes/Seagate/generated-data/html/tokenized/2.tokenized',\n",
       "  b'/Volumes/Seagate/generated-data/expected_json/tokenized/2.tokenized'),\n",
       " (b'/Volumes/Seagate/generated-data/html/tokenized/3.tokenized',\n",
       "  b'/Volumes/Seagate/generated-data/expected_json/tokenized/3.tokenized'),\n",
       " (b'/Volumes/Seagate/generated-data/html/tokenized/4.tokenized',\n",
       "  b'/Volumes/Seagate/generated-data/expected_json/tokenized/4.tokenized')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(html_filenames) # (html_filenames, json_filenames))\n",
    "list(dataset.as_numpy_iterator())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T21:15:42.348342Z",
     "start_time": "2020-10-22T21:15:42.343578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/Seagate/generated-data/expected_json/tokenized/0.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/1.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/2.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/3.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/4.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/5.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/6.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/7.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/8.tokenized',\n",
       " '/Volumes/Seagate/generated-data/expected_json/tokenized/9.tokenized']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.575832Z",
     "start_time": "2020-10-13T01:44:20.572222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2408"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.688423Z",
     "start_time": "2020-10-13T01:44:20.587039Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_ds = get_datasets('/Volumes/Seagate/generated-data-combined-html-json/*.combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.698536Z",
     "start_time": "2020-10-13T01:44:20.690599Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_len(ds):\n",
    "    cardinality = tf.data.experimental.cardinality(ds)\n",
    "    if cardinality == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "        print('INFINITE_CARDINALITY')\n",
    "        return\n",
    "    elif cardinality < 0:\n",
    "        print(f'Negative cardinality: {cardinality}')\n",
    "        \n",
    "    count = 0\n",
    "    for x in combined_ds:\n",
    "        count += 1\n",
    "    print(f'Counted dataset length: {count}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.705644Z",
     "start_time": "2020-10-13T01:44:20.702553Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_print(ds):\n",
    "    dataset_len(ds)\n",
    "    print('Dataset first element: \\n')\n",
    "    DS_HEAD_LEN = 1\n",
    "    for x in ds.take(DS_HEAD_LEN):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.846734Z",
     "start_time": "2020-10-13T01:44:20.707838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10, 2, 2408), dtype=int32, numpy=\n",
       " array([[[[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]]], dtype=int32)>)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.convert_to_tensor(list(combined_ds.as_numpy_iterator()))\n",
    "encoder_values = t[0, :, 0, :]\n",
    "encoder_values = encoder_values[:, :, np.newaxis]\n",
    "encoder_values = tf.cast(encoder_values, dtype=tf.int32)\n",
    "decoder_values = t[0, :, 1, :]\n",
    "decoder_values = decoder_values[:, :, np.newaxis]\n",
    "\n",
    "t, encoder_values, decoder_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.853100Z",
     "start_time": "2020-10-13T01:44:20.849045Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function assumes the size of the embeddings is 1 per token\n",
    "def get_sequence_lengths(embeddings):\n",
    "    axis_removed_embeddings = np.squeeze(embeddings)\n",
    "    sequence_lengths = np.zeros(embeddings.shape[0])\n",
    "    max_len = embeddings.shape[1]\n",
    "    index = 0\n",
    "    for xs in axis_removed_embeddings:\n",
    "        for i, y in enumerate(xs):\n",
    "            if y != 0:\n",
    "                sequence_lengths[index] = max_len - i\n",
    "                index += 1\n",
    "                break\n",
    "\n",
    "    return sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.978075Z",
     "start_time": "2020-10-13T01:44:20.855338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "Filename                                      First few bytes                                  lengths\n",
      "7.combined: 263 293 217 253 569 570 366 156 521 298:306 152 368 402 120 298 509 120 230 120    938:1244\n",
      "0.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 420 120 230 120 528    1322:805\n",
      "4.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 513 120 230 120 191    1456:990\n",
      "1.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 356 120 230 120 264    1467:1001\n",
      "9.combined: 263 293 217 492 237 369 470 308 492 237:306 152 368 402 120 369 470 120 230 120    2408:1451\n",
      "2.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 180 120 230 120 322    1321:804\n",
      "3.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 417 473 120 230 120    1460:994\n",
      "5.combined: 263 293 217 253 569 570 366 156 521 313:306 152 368 402 120 313 120 230 120 433    938:1244\n",
      "8.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 234 556 120 230 120    1324:807\n",
      "6.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 383 120 230 120 186    1460:994\n"
     ]
    }
   ],
   "source": [
    "def check_enc_dec(file_pattern, enc, dec):\n",
    "    \n",
    "    filenames = list(tf.data.Dataset.list_files(file_pattern, seed=10).as_numpy_iterator())\n",
    "    filenames = [fn.decode('utf-8') for fn in filenames]\n",
    "\n",
    "    def embedding_values(e):\n",
    "        return np.squeeze(e)\n",
    "            \n",
    "    enc_values = embedding_values(enc)\n",
    "    enc_values = [np.flip(xs) for xs in enc_values]\n",
    "    enc_values = [list(xs.astype(str)) for xs in enc_values]\n",
    "    \n",
    "    dec_values = embedding_values(dec)\n",
    "    dec_values = [np.flip(xs) for xs in dec_values]\n",
    "    dec_values = [list(xs.astype(str)) for xs in dec_values]\n",
    "\n",
    "    enc_lengths = get_sequence_lengths(enc)\n",
    "    dec_lengths = get_sequence_lengths(dec)\n",
    "    \n",
    "    print('Values:')\n",
    "    print('Filename                                      First few bytes                                  lengths')\n",
    "    for i, filename in enumerate(filenames):\n",
    "        fn = filename.split(os.sep)[-1]\n",
    "        print('{}: {}:{}    {}:{}'.format(fn, ' '.join(enc_values[i][:10]), ' '.join(dec_values[i][:10]),\n",
    "                                          int(enc_lengths[i]), int(dec_lengths[i])))\n",
    "\n",
    "check_enc_dec('/Volumes/Seagate/generated-data-combined-html-json/*.combined',\n",
    "              encoder_values, decoder_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.008861Z",
     "start_time": "2020-10-13T01:44:20.985266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename                                      First few bytes                                  lengths\n",
      "0.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 420 120 230 120 528    1322:805\n",
      "1.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 356 120 230 120 264    1467:1001\n",
      "2.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 180 120 230 120 322    1321:804\n",
      "3.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 417 473 120 230 120    1460:994\n",
      "4.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 513 120 230 120 191    1456:990\n",
      "5.combined: 263 293 217 253 569 570 366 156 521 313:306 152 368 402 120 313 120 230 120 433    938:1244\n",
      "6.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 383 120 230 120 186    1460:994\n",
      "7.combined: 263 293 217 253 569 570 366 156 521 298:306 152 368 402 120 298 509 120 230 120    938:1244\n",
      "8.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 234 556 120 230 120    1324:807\n",
      "9.combined: 263 293 217 492 237 369 470 308 492 237:306 152 368 402 120 369 470 120 230 120    2408:1451\n"
     ]
    }
   ],
   "source": [
    "def check_data_files(file_pattern):\n",
    "    enc_lengths = dec_lengths = []\n",
    "    print('Filename                                      First few bytes                                  lengths')\n",
    "    for i, fn in enumerate(iglob(file_pattern)):\n",
    "        with open(fn, 'r') as f:\n",
    "            line = f.read()\n",
    "        parts = line.split(':')\n",
    "        values = [xs.split() for xs in parts]\n",
    "        values = [[str(x) for x in xs] for xs in values]\n",
    "        enc_len, dec_len = [len(x) for x in values]\n",
    "        enc_lengths.append(enc_len)\n",
    "        dec_lengths.append(dec_len)\n",
    "        filename = fn.split(os.sep)[-1]\n",
    "        \n",
    "        print(f'{filename}: {\" \".join(values[0][:10])}:{\" \".join(values[1][:10])}    {enc_len}:{dec_len}')\n",
    "        \n",
    "check_data_files('/Volumes/Seagate/generated-data-combined-html-json/*.combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.079572Z",
     "start_time": "2020-10-13T01:44:21.012636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 938., 1322., 1456., 1467., 2408., 1321., 1460.,  938., 1324.,\n",
       "        1460.]),\n",
       " array([1244.,  805.,  990., 1001., 1451.,  804.,  994., 1244.,  807.,\n",
       "         994.]))"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence_lengths(encoder_values), get_sequence_lengths(decoder_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.087320Z",
     "start_time": "2020-10-13T01:44:21.082489Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]]], dtype=int32)>)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_values, decoder_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.128630Z",
     "start_time": "2020-10-13T01:44:21.090398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(882,\n",
       " [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  97,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  1434,\n",
       "  1595,\n",
       "  1814,\n",
       "  1872,\n",
       "  3101,\n",
       "  4218,\n",
       "  4952,\n",
       "  5493,\n",
       "  5706,\n",
       "  6743,\n",
       "  6839,\n",
       "  7226,\n",
       "  8244,\n",
       "  8772,\n",
       "  9235,\n",
       "  9482,\n",
       "  9621,\n",
       "  10726,\n",
       "  10783,\n",
       "  11082,\n",
       "  11808,\n",
       "  11962,\n",
       "  13020,\n",
       "  13126,\n",
       "  13576,\n",
       "  13816,\n",
       "  13832,\n",
       "  14192,\n",
       "  14234,\n",
       "  15114,\n",
       "  15161,\n",
       "  15299,\n",
       "  15436,\n",
       "  15644,\n",
       "  16992,\n",
       "  17213,\n",
       "  17355,\n",
       "  18004,\n",
       "  18006,\n",
       "  18169,\n",
       "  18533,\n",
       "  18904,\n",
       "  19292,\n",
       "  20031,\n",
       "  20146,\n",
       "  20977,\n",
       "  21573,\n",
       "  22292,\n",
       "  22419,\n",
       "  22428,\n",
       "  22817,\n",
       "  22847,\n",
       "  24303,\n",
       "  24929,\n",
       "  25644,\n",
       "  25770,\n",
       "  26328,\n",
       "  26372,\n",
       "  26619,\n",
       "  27599,\n",
       "  27820,\n",
       "  29461,\n",
       "  30270,\n",
       "  30276,\n",
       "  30391,\n",
       "  30624,\n",
       "  30709,\n",
       "  30783,\n",
       "  31038,\n",
       "  31498,\n",
       "  31828,\n",
       "  31846,\n",
       "  32376,\n",
       "  32499,\n",
       "  33051,\n",
       "  33444,\n",
       "  33627,\n",
       "  34170,\n",
       "  35716,\n",
       "  36213,\n",
       "  36375,\n",
       "  36582,\n",
       "  36919,\n",
       "  37557,\n",
       "  37928,\n",
       "  38295,\n",
       "  38898,\n",
       "  39062,\n",
       "  39272,\n",
       "  40242,\n",
       "  40277,\n",
       "  40296,\n",
       "  41099,\n",
       "  41130,\n",
       "  42327,\n",
       "  42498,\n",
       "  42921,\n",
       "  43274,\n",
       "  44285,\n",
       "  44927,\n",
       "  45641,\n",
       "  46741,\n",
       "  47250,\n",
       "  47629,\n",
       "  47834,\n",
       "  47983,\n",
       "  48081,\n",
       "  48306,\n",
       "  48403,\n",
       "  48937,\n",
       "  49097,\n",
       "  49438,\n",
       "  49588,\n",
       "  49629,\n",
       "  49916,\n",
       "  49919,\n",
       "  50504,\n",
       "  50582,\n",
       "  51212,\n",
       "  51399,\n",
       "  51554,\n",
       "  51590,\n",
       "  52090,\n",
       "  52406,\n",
       "  52773,\n",
       "  52876,\n",
       "  53710,\n",
       "  54220,\n",
       "  54792,\n",
       "  55210,\n",
       "  55881,\n",
       "  55915,\n",
       "  56321,\n",
       "  56384,\n",
       "  57149,\n",
       "  57420,\n",
       "  57525,\n",
       "  59092,\n",
       "  59095,\n",
       "  59842,\n",
       "  60072,\n",
       "  60569,\n",
       "  61027,\n",
       "  61123,\n",
       "  61317,\n",
       "  61322,\n",
       "  61971,\n",
       "  62346,\n",
       "  62606,\n",
       "  63236,\n",
       "  63532,\n",
       "  63732,\n",
       "  63942,\n",
       "  64564,\n",
       "  64604,\n",
       "  65421,\n",
       "  65743,\n",
       "  66375,\n",
       "  67093,\n",
       "  67166,\n",
       "  67583,\n",
       "  68141,\n",
       "  68347,\n",
       "  68434,\n",
       "  68933,\n",
       "  69181,\n",
       "  69319,\n",
       "  70219,\n",
       "  70247,\n",
       "  71429,\n",
       "  71656,\n",
       "  72128,\n",
       "  72498,\n",
       "  72534,\n",
       "  74112,\n",
       "  74439,\n",
       "  74803,\n",
       "  75812,\n",
       "  75953,\n",
       "  76468,\n",
       "  76915,\n",
       "  77886,\n",
       "  78601,\n",
       "  78839,\n",
       "  79132,\n",
       "  79177,\n",
       "  79366,\n",
       "  79425,\n",
       "  80428,\n",
       "  81333,\n",
       "  81842,\n",
       "  81844,\n",
       "  82204,\n",
       "  82482,\n",
       "  82891,\n",
       "  83347,\n",
       "  83439,\n",
       "  83997,\n",
       "  84442,\n",
       "  84469,\n",
       "  84581,\n",
       "  84615,\n",
       "  84790,\n",
       "  85196,\n",
       "  85198,\n",
       "  86279,\n",
       "  86306,\n",
       "  86457,\n",
       "  86749,\n",
       "  87082,\n",
       "  87811,\n",
       "  88422,\n",
       "  88488,\n",
       "  88512,\n",
       "  88543,\n",
       "  89991,\n",
       "  90510,\n",
       "  90789,\n",
       "  90937,\n",
       "  91075,\n",
       "  91504,\n",
       "  92111,\n",
       "  92165,\n",
       "  93873,\n",
       "  94370,\n",
       "  95123,\n",
       "  96458,\n",
       "  96510,\n",
       "  96527,\n",
       "  96727,\n",
       "  97032,\n",
       "  97318,\n",
       "  97482,\n",
       "  99258,\n",
       "  99458,\n",
       "  99716,\n",
       "  122093,\n",
       "  132641,\n",
       "  148418,\n",
       "  177557,\n",
       "  187515,\n",
       "  227301,\n",
       "  234900,\n",
       "  238882,\n",
       "  270938,\n",
       "  272016,\n",
       "  300755,\n",
       "  316927,\n",
       "  338596,\n",
       "  339857,\n",
       "  340324,\n",
       "  369204,\n",
       "  377509,\n",
       "  378195,\n",
       "  381169,\n",
       "  386498,\n",
       "  403410,\n",
       "  407448,\n",
       "  409482,\n",
       "  437120,\n",
       "  453893,\n",
       "  455201,\n",
       "  460872,\n",
       "  461544,\n",
       "  476401,\n",
       "  503677,\n",
       "  545342,\n",
       "  585967,\n",
       "  589781,\n",
       "  630399,\n",
       "  638422,\n",
       "  643768,\n",
       "  653019,\n",
       "  655395,\n",
       "  663332,\n",
       "  712355,\n",
       "  769358,\n",
       "  776199,\n",
       "  778196,\n",
       "  789895,\n",
       "  790845,\n",
       "  796301,\n",
       "  799889,\n",
       "  858877,\n",
       "  878891,\n",
       "  895874,\n",
       "  896261,\n",
       "  952349,\n",
       "  952813,\n",
       "  963810,\n",
       "  977999,\n",
       "  990350,\n",
       "  1003012,\n",
       "  1619135,\n",
       "  2244298,\n",
       "  3395670,\n",
       "  3854652,\n",
       "  4566181,\n",
       "  6201368,\n",
       "  7505635,\n",
       "  7907276,\n",
       "  8655107,\n",
       "  8749571,\n",
       "  9129829,\n",
       "  999999999])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(enc, dec):\n",
    "    def build_vocab(values):\n",
    "        values_set = set()        \n",
    "        for v in values:\n",
    "            values_set.update(np.squeeze(v))\n",
    "        return values_set\n",
    "    \n",
    "    enc_set = build_vocab(enc)\n",
    "    dec_set = build_vocab(dec)\n",
    "    values_set = enc_set | dec_set\n",
    "    \n",
    "    return sorted(list(values_set))\n",
    "\n",
    "encoder_values = np.squeeze(encoder_values)\n",
    "decoder_values = np.squeeze(decoder_values)\n",
    "vocab = build_vocabulary(encoder_values, decoder_values)\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 4  # 4 float32 values for each token of input\n",
    "vocab_size, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.136886Z",
     "start_time": "2020-10-13T01:44:21.130856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (10, 2408), dtype('int32'))"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder_values), encoder_values.shape, encoder_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:25.197352Z",
     "start_time": "2020-10-13T01:44:21.140581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10, 2408)\n",
      "<class 'numpy.ndarray'> (10, 2408)\n"
     ]
    }
   ],
   "source": [
    "vocab_array = np.array(vocab)\n",
    "\n",
    "def build_indices(values):\n",
    "    return np.squeeze(np.array([[np.where(vocab == x) \n",
    "                                    for x in value] \n",
    "                                for value in values]))\n",
    "\n",
    "encoder_indices = build_indices(encoder_values)\n",
    "decoder_indices = build_indices(decoder_values)\n",
    "print(type(encoder_indices), encoder_indices.shape)\n",
    "print(type(decoder_indices), decoder_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:14:15.052669Z",
     "start_time": "2020-10-13T02:14:14.714647Z"
    }
   },
   "outputs": [],
   "source": [
    "# All of this code is taken from Aurelien Geron's\n",
    "# notebook which accompanies the book\n",
    "# Handson Machine Learning with Scikit-Learn and Tensorflow.\n",
    "# You can find it here:\n",
    "# https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb\n",
    "#\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(4, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(4)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:14:15.861155Z",
     "start_time": "2020-10-13T02:14:15.849190Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"adam\",\n",
    "              run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:14:16.960096Z",
     "start_time": "2020-10-13T02:14:16.953741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_60\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_154 (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_153 (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_31 (Embedding)        (None, None, 4)      3528        input_153[0][0]                  \n",
      "                                                                 input_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                  [(None, 4), (None, 4 144         embedding_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_155 (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_35 (BasicDecoder) (BasicDecoderOutput( 4554        embedding_31[1][0]               \n",
      "                                                                 lstm_61[0][1]                    \n",
      "                                                                 lstm_61[0][2]                    \n",
      "                                                                 input_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_32 (TensorF [(None, None, 882)]  0           basic_decoder_35[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 8,226\n",
      "Trainable params: 8,226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:16:03.058703Z",
     "start_time": "2020-10-13T02:15:43.880120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad310eba00>"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_indices_shifted = np.c_[np.zeros((decoder_indices.shape[0], 1)),\n",
    "                                decoder_indices[:, :-1]]\n",
    "# print(encoder_indices.shape)\n",
    "# print(decoder_indices.shape)\n",
    "sequence_lengths = np.full([decoder_indices.shape[0]], decoder_indices.shape[1])\n",
    "# print(sequence_lengths.shape)\n",
    "# print(sequence_lengths[:5])\n",
    "# print(type(sequence_lengths))\n",
    "model.fit([encoder_indices, decoder_indices_shifted, sequence_lengths], \n",
    "          decoder_indices,\n",
    "          epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.117901Z",
     "start_time": "2020-11-01T22:03:01.632331Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.132850Z",
     "start_time": "2020-11-01T22:03:04.120432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.],\n",
       "        [ 1.,  1.]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.140517Z",
     "start_time": "2020-11-01T22:03:04.135648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.pow(2).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.147977Z",
     "start_time": "2020-11-01T22:03:04.143308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.155534Z",
     "start_time": "2020-11-01T22:03:04.150054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32,\n",
       " tensor([[0.7328, 0.7336, 0.2755, 0.2053],\n",
       "         [0.1659, 0.9661, 0.1790, 0.5412],\n",
       "         [0.4934, 0.8503, 0.3691, 0.7660],\n",
       "         [0.8465, 0.3042, 0.8909, 0.3216]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(4, 4)  # 4x4 random tensor\n",
    "t.dtype, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.162343Z",
     "start_time": "2020-11-01T22:03:04.157569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7328, 0.7336, 0.2755, 0.2053, 0.1659, 0.9661, 0.1790, 0.5412],\n",
       "        [0.4934, 0.8503, 0.3691, 0.7660, 0.8465, 0.3042, 0.8909, 0.3216]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = t.view(2, 8)\n",
    "tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.168898Z",
     "start_time": "2020-11-01T22:03:04.164313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.storage().data_ptr() == tv.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.178645Z",
     "start_time": "2020-11-01T22:03:04.173614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.1400), tensor(3.1400))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv[0][0] = 3.14\n",
    "t[0][0], tv[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.187735Z",
     "start_time": "2020-11-01T22:03:04.181791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 1], [1, 1], [2, 2]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.197040Z",
     "start_time": "2020-11-01T22:03:04.191664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.209539Z",
     "start_time": "2020-11-01T22:03:04.202414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [3],\n",
       "        [5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1], [3], [5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.219252Z",
     "start_time": "2020-11-01T22:03:04.212735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [3, 3, 3, 3, 3, 3, 3],\n",
       "        [5, 5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.226378Z",
     "start_time": "2020-11-01T22:03:04.221355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [3, 3, 3, 3, 3, 3, 3],\n",
       "        [5, 5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand_as(torch.rand(3, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.239807Z",
     "start_time": "2020-11-01T22:03:04.228589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 5],\n",
       "        [2, 4, 6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "torch.movedim(x, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:03:04.246117Z",
     "start_time": "2020-11-01T22:03:04.242059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:07:11.516907Z",
     "start_time": "2020-11-01T22:07:11.512236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 5],\n",
       "        [2, 4, 6]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T22:09:03.597297Z",
     "start_time": "2020-11-01T22:09:03.592224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.movedim(x, 1, 0) == x.t()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "544.85px",
    "left": "1016.8px",
    "right": "20px",
    "top": "120px",
    "width": "259.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
