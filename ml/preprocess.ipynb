{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.152215Z",
     "start_time": "2020-10-07T17:22:34.731220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.164513Z",
     "start_time": "2020-10-07T17:22:38.154633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:55:10.654691Z",
     "start_time": "2020-10-07T23:55:10.630538Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import iglob\n",
    "\n",
    "html_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/html/encoded/*.unescaped.encoded')))\n",
    "json_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/expected_json/encoded/*.expected_json.encoded')))\n",
    "assert(len(html_filenames) == len(json_filenames))\n",
    "combined_filenames = zip(html_filenames, json_filenames)\n",
    "\n",
    "for html_fn, json_fn in combined_filenames:\n",
    "    # print(html_fn, '\\n\\t', json_fn)\n",
    "    with open(html_fn, 'r') as f:\n",
    "        html_data = f.read()\n",
    "    with open(json_fn, 'r') as f:\n",
    "        json_data = f.read()\n",
    "\n",
    "    with open(os.path.join('/Volumes/Seagate/generated-data-combined-html-json',\n",
    "                           html_fn.split(os.sep)[-1].split('.')[0] + '.combined'), 'w') as f:\n",
    "        f.write(html_data + ' : ' + json_data)\n",
    "\n",
    "\n",
    "def read_file(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def write_file(fn, data):\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    write_file(dst, read_file(src))\n",
    "    \n",
    "\n",
    "copy_file('/Volumes/Seagate/generated-data/expected_json/encoded/max_encoded_file_token_len',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len')\n",
    "copy_file('/Volumes/Seagate/generated-data/tokens',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.190812Z",
     "start_time": "2020-10-07T17:22:38.187535Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len', 'r') as f:\n",
    "    line = f.read()\n",
    "    key, value = line.split('=')\n",
    "    assert(key == 'max_encoded_file_token_len')\n",
    "    max_encoded_file_token_len = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:57:07.416473Z",
     "start_time": "2020-10-07T20:57:07.409184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the data:\n",
    "#   - During training:\n",
    "#     - We're planning on using 10K generated files.\n",
    "#       Average file size around 9K\n",
    "#       90M X 4 (for uint32 numbers) = 360MB full HTML training data.\n",
    "#       Also some more memory needed to hold JSON training data.\n",
    "#       We can decrease from 10K files to 5K generated files, \n",
    "#       or increase the memory reserved for this application to\n",
    "#       hold this entire data in memory.\n",
    "#     - So we can shuffle this data as a part of the model.\n",
    "#       It is good to shuffle at least per epoch so the model\n",
    "#       is not biased.\n",
    "#     - You can specify:\n",
    "#       dataset = dataset.shuffle(buffer_size=100,    # prefilled buffer to speed up shuffling\n",
    "#                                 random_seed = 10,   # random seed set to ensure repeatability\n",
    "#                                 reshuffle_each_iteration=True)  # True by default. Set to False for debugging.\n",
    "#   - During validation/testing:\n",
    "#     - No need to hold the entire dataset in memory to do this since\n",
    "#       we can apply the model for validation testing on each file.\n",
    "\n",
    "batch_size = 32\n",
    "num_prefetch = 1\n",
    "def get_datasets(filepath):\n",
    "    def get_text_line_dataset(filepath):\n",
    "       return tf.data.TextLineDataset(filepath)\n",
    "\n",
    "    def get_combined(line):\n",
    "        # print(type(line))\n",
    "        return tf.strings.split(line, ':')\n",
    "    \n",
    "    def unicode_to_ascii(unicode):\n",
    "        return tf.strings.to_number(unicode, out_type=tf.int32)\n",
    "\n",
    "    def pad(ints):\n",
    "        # print(type(ints))\n",
    "        t = ints.to_tensor(shape=(2, max_encoded_file_token_len))\n",
    "        return t\n",
    "\n",
    "    def reverse(padded):\n",
    "        # print(type(padded))\n",
    "        return tf.reverse(padded, axis=[1])\n",
    "    \n",
    "    n_readers = 5\n",
    "    dataset = tf.data.Dataset.list_files(filepath, seed=10) \\\n",
    "                             .interleave(get_text_line_dataset, cycle_length=n_readers) \\\n",
    "                             .map(get_combined) \\\n",
    "                             .map(tf.strings.split) \\\n",
    "                             .map(unicode_to_ascii) \\\n",
    "                             .map(int) \\\n",
    "                             .map(pad) \\\n",
    "                             .map(reverse) \\\n",
    "                             .batch(batch_size) \\\n",
    "                             .prefetch(num_prefetch)\n",
    "\n",
    "    #for x in dataset:\n",
    "    #    print(x)\n",
    "    #    break\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T20:57:10.745580Z",
     "start_time": "2020-10-07T20:57:10.743282Z"
    }
   },
   "outputs": [],
   "source": [
    "# At the end, we will batch and prefetch\n",
    "# return dataset.batch(batch_size).prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T00:07:28.246868Z",
     "start_time": "2020-10-08T00:07:28.159459Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab_size(filename):\n",
    "    # TODO:\n",
    "    # Let's fix this for now. Later we should write this information\n",
    "    # to a file and read it from there.\n",
    "    # The vocab size is the number of unique tokens (so token size)\n",
    "    return 485\n",
    "\n",
    "combined_ds = get_datasets('/Volumes/Seagate/generated-data-combined-html-json/*.combined')\n",
    "vocab_size = get_vocab_size('/Volumes/Seagate/generated-data-combined-html-json/tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T00:05:55.846100Z",
     "start_time": "2020-10-08T00:05:55.841977Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_len(ds):\n",
    "    cardinality = tf.data.experimental.cardinality(ds)\n",
    "    if cardinality == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "        print('INFINITE_CARDINALITY')\n",
    "        return\n",
    "    elif cardinality < 0:\n",
    "        print(f'Negative cardinality: {cardinality}')\n",
    "        \n",
    "    count = 0\n",
    "    for x in combined_ds:\n",
    "        count += 1\n",
    "    print(f'Counted dataset length: {count}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T00:05:56.868950Z",
     "start_time": "2020-10-08T00:05:56.865462Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_print(ds):\n",
    "    dataset_len(ds)\n",
    "    print('Dataset first element: \\n')\n",
    "    DS_HEAD_LEN = 1\n",
    "    for x in ds.take(DS_HEAD_LEN):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T00:28:15.510484Z",
     "start_time": "2020-10-08T00:28:15.396963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10, 2, 2408), dtype=int32, numpy=\n",
       " array([[[[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[431, 516, 345, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408), dtype=int32, numpy=\n",
       " array([[  0,   0,   0, ..., 217, 293, 263],\n",
       "        [  0,   0,   0, ..., 217, 293, 263],\n",
       "        [  0,   0,   0, ..., 237, 293, 263],\n",
       "        ...,\n",
       "        [431, 516, 345, ..., 217, 293, 263],\n",
       "        [  0,   0,   0, ..., 217, 293, 263],\n",
       "        [  0,   0,   0, ..., 217, 293, 263]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408), dtype=int32, numpy=\n",
       " array([[  0,   0,   0, ..., 368, 152, 306],\n",
       "        [  0,   0,   0, ..., 368, 152, 306],\n",
       "        [  0,   0,   0, ..., 368, 152, 306],\n",
       "        ...,\n",
       "        [  0,   0,   0, ..., 368, 152, 306],\n",
       "        [  0,   0,   0, ..., 368, 152, 306],\n",
       "        [  0,   0,   0, ..., 368, 152, 306]], dtype=int32)>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.convert_to_tensor(list(combined_ds.as_numpy_iterator()))\n",
    "encoder_embeddings = t[0, :, 0, :]\n",
    "decoder_embeddings = t[0, :, 1, :]\n",
    "t, encoder_embeddings, decoder_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T00:28:45.294693Z",
     "start_time": "2020-10-08T00:28:45.261079Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_6 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [10, 2408]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-a73491aeba19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mencoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2617\u001b[0;31m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[1;32m   2618\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[1;32m   2619\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[1;32m    177\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_6 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [10, 2408]"
     ]
    }
   ],
   "source": [
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, \n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_output_state, final_sequence_lengths = \\\n",
    "    decoder(decoder_embeddings, initial_state=encoder_state,\n",
    "            sequence_length=sequence_lengths)\n",
    "\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "                    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:44:34.314340Z",
     "start_time": "2020-10-05T20:44:34.303531Z"
    }
   },
   "outputs": [],
   "source": [
    "text_vectorizer = \\\n",
    "    preprocessing.TextVectorization(max_tokens=None, standardize=None,\n",
    "                                   split=\"whitespace\", ngrams=None,\n",
    "                                   output_mode=\"int\", output_sequence_length=max_data_len,\n",
    "                                   pad_to_max_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:44:35.131646Z",
     "start_time": "2020-10-05T20:44:35.119018Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a1da5003c095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madapted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adapted_data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "adapted_data = text_vectorizer.adapt(data.batch(64))\n",
    "print('adapted_data:', adapted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T17:58:35.916839Z",
     "start_time": "2020-10-05T17:58:35.672440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '1', '10', '11', '12,', '13,', '14,', '15,', '16', '17,', '18,', '19,', '2', '20,', '21', '3', '4', '5', '6', '7', '8', '9', '[UNK]']\n",
      "test_output: tf.Tensor([[0.0213149]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vocab = text_vectorizer.get_vocabulary()\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "test_data = tf.constant([\"12 5 9 20 52\"])\n",
    "test_output = model(test_data)\n",
    "\n",
    "print(\"test_output:\", test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T17:49:04.095432Z",
     "start_time": "2020-10-05T17:49:03.788556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'the', 'side', 'you', 'with', 'will', 'wider', 'them', 'than', 'sky', 'put', 'other', 'one', 'is', 'for', 'ease', 'contain', 'by', 'brain', 'beside', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"int\" output_mode\n",
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\")\n",
    "# Index the vocabulary via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "\n",
    "# You can retrieve the vocabulary we indexed via get_vocabulary()\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Create an Embedding + LSTM model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the model on test data (which includes unknown tokens)\n",
    "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
    "test_output = model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
