{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:28:34.642078Z",
     "start_time": "2020-10-09T01:28:34.638734Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:28:36.010118Z",
     "start_time": "2020-10-09T01:28:36.006220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:15.685386Z",
     "start_time": "2020-10-09T01:25:15.661744Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import iglob\n",
    "\n",
    "html_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/html/encoded/*.unescaped.encoded')))\n",
    "json_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/expected_json/encoded/*.expected_json.encoded')))\n",
    "assert(len(html_filenames) == len(json_filenames))\n",
    "combined_filenames = zip(html_filenames, json_filenames)\n",
    "\n",
    "for html_fn, json_fn in combined_filenames:\n",
    "    # print(html_fn, '\\n\\t', json_fn)\n",
    "    with open(html_fn, 'r') as f:\n",
    "        html_data = f.read()\n",
    "    with open(json_fn, 'r') as f:\n",
    "        json_data = f.read()\n",
    "\n",
    "    with open(os.path.join('/Volumes/Seagate/generated-data-combined-html-json',\n",
    "                           html_fn.split(os.sep)[-1].split('.')[0] + '.combined'), 'w') as f:\n",
    "        f.write(html_data + ' : ' + json_data)\n",
    "\n",
    "\n",
    "def read_file(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def write_file(fn, data):\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    write_file(dst, read_file(src))\n",
    "    \n",
    "\n",
    "copy_file('/Volumes/Seagate/generated-data/expected_json/encoded/max_encoded_file_token_len',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len')\n",
    "copy_file('/Volumes/Seagate/generated-data/tokens',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:15.691247Z",
     "start_time": "2020-10-09T01:25:15.687955Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len', 'r') as f:\n",
    "    line = f.read()\n",
    "    key, value = line.split('=')\n",
    "    assert(key == 'max_encoded_file_token_len')\n",
    "    max_encoded_file_token_len = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T17:39:50.582287Z",
     "start_time": "2020-10-09T17:39:50.575561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the data:\n",
    "#   - During training:\n",
    "#     - We're planning on using 10K generated files.\n",
    "#       Average file size around 9K\n",
    "#       90M X 4 (for uint32 numbers) = 360MB full HTML training data.\n",
    "#       Also some more memory needed to hold JSON training data.\n",
    "#       We can decrease from 10K files to 5K generated files, \n",
    "#       or increase the memory reserved for this application to\n",
    "#       hold this entire data in memory.\n",
    "#     - So we can shuffle this data as a part of the model.\n",
    "#       It is good to shuffle at least per epoch so the model\n",
    "#       is not biased.\n",
    "#     - You can specify:\n",
    "#       dataset = dataset.shuffle(buffer_size=100,    # prefilled buffer to speed up shuffling\n",
    "#                                 random_seed = 10,   # random seed set to ensure repeatability\n",
    "#                                 reshuffle_each_iteration=True)  # True by default. Set to False for debugging.\n",
    "#   - During validation/testing:\n",
    "#     - No need to hold the entire dataset in memory to do this since\n",
    "#       we can apply the model for validation testing on each file.\n",
    "\n",
    "batch_size = 32\n",
    "num_prefetch = 1\n",
    "def get_datasets(filepath):\n",
    "    def get_text_line_dataset(filepath):\n",
    "       return tf.data.TextLineDataset(filepath)\n",
    "\n",
    "    def get_combined(line):\n",
    "        # print(type(line))\n",
    "        return tf.strings.split(line, ':')\n",
    "    \n",
    "    def unicode_to_ascii(unicode):\n",
    "        return tf.strings.to_number(unicode, out_type=tf.int32)\n",
    "\n",
    "    def pad(ints):\n",
    "        # print(type(ints))\n",
    "        t = ints.to_tensor(shape=(2, max_encoded_file_token_len))\n",
    "        return t\n",
    "\n",
    "    def reverse(padded):\n",
    "        # print(type(padded))\n",
    "        return tf.reverse(padded, axis=[1])\n",
    "    \n",
    "    n_readers = 5\n",
    "    dataset = tf.data.Dataset.list_files(filepath, seed=10) \\\n",
    "                             .interleave(get_text_line_dataset, cycle_length=n_readers) \\\n",
    "                             .map(get_combined) \\\n",
    "                             .map(tf.strings.split) \\\n",
    "                             .map(unicode_to_ascii) \\\n",
    "                             .map(int) \\\n",
    "                             .map(pad) \\\n",
    "                             .map(reverse) \\\n",
    "                             .batch(batch_size) \\\n",
    "                             .prefetch(num_prefetch)\n",
    "\n",
    "    #for x in dataset:\n",
    "    #    print(x)\n",
    "    #    break\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:15.815733Z",
     "start_time": "2020-10-09T01:25:15.702957Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab_size(filename):\n",
    "    # TODO:\n",
    "    # Let's fix this for now. Later we should write this information\n",
    "    # to a file and read it from there.\n",
    "    # The vocab size is the number of unique tokens (so token size)\n",
    "    return 485\n",
    "\n",
    "combined_ds = get_datasets('/Volumes/Seagate/generated-data-combined-html-json/*.combined')\n",
    "vocab_size = get_vocab_size('/Volumes/Seagate/generated-data-combined-html-json/tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:15.823224Z",
     "start_time": "2020-10-09T01:25:15.819216Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_len(ds):\n",
    "    cardinality = tf.data.experimental.cardinality(ds)\n",
    "    if cardinality == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "        print('INFINITE_CARDINALITY')\n",
    "        return\n",
    "    elif cardinality < 0:\n",
    "        print(f'Negative cardinality: {cardinality}')\n",
    "        \n",
    "    count = 0\n",
    "    for x in combined_ds:\n",
    "        count += 1\n",
    "    print(f'Counted dataset length: {count}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:15.829307Z",
     "start_time": "2020-10-09T01:25:15.825859Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_print(ds):\n",
    "    dataset_len(ds)\n",
    "    print('Dataset first element: \\n')\n",
    "    DS_HEAD_LEN = 1\n",
    "    for x in ds.take(DS_HEAD_LEN):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:15.944333Z",
     "start_time": "2020-10-09T01:25:15.831298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10, 2, 2408), dtype=int32, numpy=\n",
       " array([[[[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]]], dtype=int32)>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.convert_to_tensor(list(combined_ds.as_numpy_iterator()))\n",
    "encoder_embeddings = t[0, :, 0, :]\n",
    "encoder_embeddings = encoder_embeddings[:, :, np.newaxis]\n",
    "encoder_embeddings = tf.cast(encoder_embeddings, dtype=tf.int32)\n",
    "decoder_embeddings = t[0, :, 1, :]\n",
    "decoder_embeddings = decoder_embeddings[:, :, np.newaxis]\n",
    "t, encoder_embeddings, decoder_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T17:38:22.422270Z",
     "start_time": "2020-10-09T17:38:22.411076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'/Volumes/Seagate/generated-data-combined-html-json/5.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/0.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/3.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/8.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/7.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/6.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/1.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/2.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/9.combined',\n",
       " b'/Volumes/Seagate/generated-data-combined-html-json/4.combined']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1 = tf.data.Dataset.list_files('/Volumes/Seagate/generated-data-combined-html-json/*.combined')\n",
    "list(ds1.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T17:33:13.491666Z",
     "start_time": "2020-10-09T17:33:13.486658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n",
      "[528 120 230 120 420 120 402 368 152 306]\n"
     ]
    }
   ],
   "source": [
    "squeezed_dec = np.squeeze(decoder_embeddings)\n",
    "for i in range(10):\n",
    "    print(squeezed_dec[0, -10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:16.010033Z",
     "start_time": "2020-10-09T01:25:15.954350Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function assumes the size of the embeddings is 1 per token\n",
    "def get_sequence_lengths(embeddings):\n",
    "    axis_removed_embeddings = np.squeeze(embeddings)\n",
    "    sequence_lengths = np.zeros(embeddings.shape[0])\n",
    "    max_len = embeddings.shape[1]\n",
    "    index = 0\n",
    "    for xs in axis_removed_embeddings:\n",
    "        for i, y in enumerate(xs):\n",
    "            if y != 0:\n",
    "                sequence_lengths[index] = max_len - i\n",
    "                index += 1\n",
    "                break\n",
    "\n",
    "    return sequence_lengths\n",
    "\n",
    "sequence_lengths = get_sequence_lengths(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:25:16.020534Z",
     "start_time": "2020-10-09T01:25:16.015267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 805.,  994., 1244., 1244., 1451.,  807., 1001.,  804.,  990.,\n",
       "        994.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.squeeze(decoder_embeddings)\n",
    "sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:39:39.752229Z",
     "start_time": "2020-10-09T01:39:39.152676Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    encoder_inputs = keras.layers.Input(shape=[max_encoded_file_token_len, 1])\n",
    "    decoder_inputs = keras.layers.Input(shape=[max_encoded_file_token_len, 1])\n",
    "    sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "    encoder = keras.layers.LSTM(512, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_state = [state_h, state_c]\n",
    "\n",
    "    sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    decoder_cell = keras.layers.LSTMCell(512)\n",
    "    output_layer = keras.layers.Dense(vocab_size)\n",
    "    decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, \n",
    "                                                 output_layer=output_layer)\n",
    "    final_outputs, final_output_state, final_sequence_lengths = \\\n",
    "        decoder(decoder_inputs, initial_state=encoder_state,\n",
    "                sequence_length=sequence_lengths)\n",
    "\n",
    "    Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "    model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "                        outputs=[Y_proba])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:39:42.562229Z",
     "start_time": "2020-10-09T01:39:42.550277Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T01:39:43.431254Z",
     "start_time": "2020-10-09T01:39:43.407590Z"
    }
   },
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-c45f363b3e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# Default to 32 for backwards compat.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m       \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \"\"\"\n\u001b[0;32m--> 877\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_bool_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_bool_casting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Default: V1-style Graph execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using a `tf.Tensor` as a Python `bool`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \" this function with @tf.function.\".format(task))\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "model.fit(encoder_embeddings, decoder_embeddings, sequence_lengths, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:44:34.314340Z",
     "start_time": "2020-10-05T20:44:34.303531Z"
    }
   },
   "outputs": [],
   "source": [
    "text_vectorizer = \\\n",
    "    preprocessing.TextVectorization(max_tokens=None, standardize=None,\n",
    "                                   split=\"whitespace\", ngrams=None,\n",
    "                                   output_mode=\"int\", output_sequence_length=max_data_len,\n",
    "                                   pad_to_max_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:44:35.131646Z",
     "start_time": "2020-10-05T20:44:35.119018Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a1da5003c095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madapted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adapted_data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "adapted_data = text_vectorizer.adapt(data.batch(64))\n",
    "print('adapted_data:', adapted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T17:58:35.916839Z",
     "start_time": "2020-10-05T17:58:35.672440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '1', '10', '11', '12,', '13,', '14,', '15,', '16', '17,', '18,', '19,', '2', '20,', '21', '3', '4', '5', '6', '7', '8', '9', '[UNK]']\n",
      "test_output: tf.Tensor([[0.0213149]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vocab = text_vectorizer.get_vocabulary()\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "test_data = tf.constant([\"12 5 9 20 52\"])\n",
    "test_output = model(test_data)\n",
    "\n",
    "print(\"test_output:\", test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T17:49:04.095432Z",
     "start_time": "2020-10-05T17:49:03.788556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'the', 'side', 'you', 'with', 'will', 'wider', 'them', 'than', 'sky', 'put', 'other', 'one', 'is', 'for', 'ease', 'contain', 'by', 'brain', 'beside', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"int\" output_mode\n",
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\")\n",
    "# Index the vocabulary via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "\n",
    "# You can retrieve the vocabulary we indexed via get_vocabulary()\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Create an Embedding + LSTM model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the model on test data (which includes unknown tokens)\n",
    "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
    "test_output = model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
