{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.531657Z",
     "start_time": "2020-10-13T01:44:20.528170Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import iglob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.538894Z",
     "start_time": "2020-10-13T01:44:20.534235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.544873Z",
     "start_time": "2020-10-13T01:44:20.541906Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.562903Z",
     "start_time": "2020-10-13T01:44:20.547528Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import iglob\n",
    "\n",
    "html_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/html/encoded/*.unescaped.encoded')))\n",
    "json_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/expected_json/encoded/*.expected_json.encoded')))\n",
    "assert(len(html_filenames) == len(json_filenames))\n",
    "combined_filenames = zip(html_filenames, json_filenames)\n",
    "\n",
    "for html_fn, json_fn in combined_filenames:\n",
    "    # print(html_fn, '\\n\\t', json_fn)\n",
    "    with open(html_fn, 'r') as f:\n",
    "        html_data = f.read()\n",
    "    with open(json_fn, 'r') as f:\n",
    "        json_data = f.read()\n",
    "\n",
    "    with open(os.path.join('/Volumes/Seagate/generated-data-combined-html-json',\n",
    "                           html_fn.split(os.sep)[-1].split('.')[0] + '.combined'), 'w') as f:\n",
    "        f.write(html_data + ' : ' + json_data)\n",
    "\n",
    "\n",
    "def read_file(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def write_file(fn, data):\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    write_file(dst, read_file(src))\n",
    "    \n",
    "\n",
    "copy_file('/Volumes/Seagate/generated-data/expected_json/encoded/max_encoded_file_token_len',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len')\n",
    "copy_file('/Volumes/Seagate/generated-data/tokens',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.568640Z",
     "start_time": "2020-10-13T01:44:20.564783Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len', 'r') as f:\n",
    "    line = f.read()\n",
    "    key, value = line.split('=')\n",
    "    assert(key == 'max_encoded_file_token_len')\n",
    "    max_encoded_file_token_len = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.575832Z",
     "start_time": "2020-10-13T01:44:20.572222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2408"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_encoded_file_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.585004Z",
     "start_time": "2020-10-13T01:44:20.578819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the data:\n",
    "#   - During training:\n",
    "#     - We're planning on using 10K generated files.\n",
    "#       Average file size around 9K\n",
    "#       90M X 4 (for uint32 numbers) = 360MB full HTML training data.\n",
    "#       Also some more memory needed to hold JSON training data.\n",
    "#       We can decrease from 10K files to 5K generated files, \n",
    "#       or increase the memory reserved for this application to\n",
    "#       hold this entire data in memory.\n",
    "#     - So we can shuffle this data as a part of the model.\n",
    "#       It is good to shuffle at least per epoch so the model\n",
    "#       is not biased.\n",
    "#     - You can specify:\n",
    "#       dataset = dataset.shuffle(buffer_size=100,    # prefilled buffer to speed up shuffling\n",
    "#                                 random_seed = 10,   # random seed set to ensure repeatability\n",
    "#                                 reshuffle_each_iteration=True)  # True by default. Set to False for debugging.\n",
    "#   - During validation/testing:\n",
    "#     - No need to hold the entire dataset in memory to do this since\n",
    "#       we can apply the model for validation testing on each file.\n",
    "\n",
    "batch_size = 32\n",
    "num_prefetch = 1\n",
    "def get_datasets(filepath):\n",
    "    def get_text_line_dataset(filepath):\n",
    "       return tf.data.TextLineDataset(filepath)\n",
    "\n",
    "    def get_combined(line):\n",
    "        # print(type(line))\n",
    "        return tf.strings.split(line, ':')\n",
    "    \n",
    "    def unicode_to_ascii(unicode):\n",
    "        return tf.strings.to_number(unicode, out_type=tf.int32)\n",
    "\n",
    "    def pad(ints):\n",
    "        # print(type(ints))\n",
    "        t = ints.to_tensor(shape=(2, max_encoded_file_token_len))\n",
    "        return t\n",
    "\n",
    "    def reverse(padded):\n",
    "        # print(type(padded))\n",
    "        return tf.reverse(padded, axis=[1])\n",
    "    \n",
    "    n_readers = 5\n",
    "    dataset = tf.data.Dataset.list_files(filepath, seed=10) \\\n",
    "                             .interleave(get_text_line_dataset, cycle_length=n_readers) \\\n",
    "                             .map(get_combined) \\\n",
    "                             .map(tf.strings.split) \\\n",
    "                             .map(unicode_to_ascii) \\\n",
    "                             .map(int) \\\n",
    "                             .map(pad) \\\n",
    "                             .map(reverse) \\\n",
    "                             .batch(batch_size) \\\n",
    "                             .prefetch(num_prefetch)\n",
    "\n",
    "    #for x in dataset:\n",
    "    #    print(x)\n",
    "    #    break\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.688423Z",
     "start_time": "2020-10-13T01:44:20.587039Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_ds = get_datasets('/Volumes/Seagate/generated-data-combined-html-json/*.combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.698536Z",
     "start_time": "2020-10-13T01:44:20.690599Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_len(ds):\n",
    "    cardinality = tf.data.experimental.cardinality(ds)\n",
    "    if cardinality == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "        print('INFINITE_CARDINALITY')\n",
    "        return\n",
    "    elif cardinality < 0:\n",
    "        print(f'Negative cardinality: {cardinality}')\n",
    "        \n",
    "    count = 0\n",
    "    for x in combined_ds:\n",
    "        count += 1\n",
    "    print(f'Counted dataset length: {count}')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.705644Z",
     "start_time": "2020-10-13T01:44:20.702553Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_print(ds):\n",
    "    dataset_len(ds)\n",
    "    print('Dataset first element: \\n')\n",
    "    DS_HEAD_LEN = 1\n",
    "    for x in ds.take(DS_HEAD_LEN):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.846734Z",
     "start_time": "2020-10-13T01:44:20.707838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10, 2, 2408), dtype=int32, numpy=\n",
       " array([[[[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 237, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]],\n",
       " \n",
       "         [[  0,   0,   0, ..., 217, 293, 263],\n",
       "          [  0,   0,   0, ..., 368, 152, 306]]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]]], dtype=int32)>)"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.convert_to_tensor(list(combined_ds.as_numpy_iterator()))\n",
    "encoder_values = t[0, :, 0, :]\n",
    "encoder_values = encoder_values[:, :, np.newaxis]\n",
    "encoder_values = tf.cast(encoder_values, dtype=tf.int32)\n",
    "decoder_values = t[0, :, 1, :]\n",
    "decoder_values = decoder_values[:, :, np.newaxis]\n",
    "\n",
    "t, encoder_values, decoder_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.853100Z",
     "start_time": "2020-10-13T01:44:20.849045Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function assumes the size of the embeddings is 1 per token\n",
    "def get_sequence_lengths(embeddings):\n",
    "    axis_removed_embeddings = np.squeeze(embeddings)\n",
    "    sequence_lengths = np.zeros(embeddings.shape[0])\n",
    "    max_len = embeddings.shape[1]\n",
    "    index = 0\n",
    "    for xs in axis_removed_embeddings:\n",
    "        for i, y in enumerate(xs):\n",
    "            if y != 0:\n",
    "                sequence_lengths[index] = max_len - i\n",
    "                index += 1\n",
    "                break\n",
    "\n",
    "    return sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:20.978075Z",
     "start_time": "2020-10-13T01:44:20.855338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      "Filename                                      First few bytes                                  lengths\n",
      "7.combined: 263 293 217 253 569 570 366 156 521 298:306 152 368 402 120 298 509 120 230 120    938:1244\n",
      "0.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 420 120 230 120 528    1322:805\n",
      "4.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 513 120 230 120 191    1456:990\n",
      "1.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 356 120 230 120 264    1467:1001\n",
      "9.combined: 263 293 217 492 237 369 470 308 492 237:306 152 368 402 120 369 470 120 230 120    2408:1451\n",
      "2.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 180 120 230 120 322    1321:804\n",
      "3.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 417 473 120 230 120    1460:994\n",
      "5.combined: 263 293 217 253 569 570 366 156 521 313:306 152 368 402 120 313 120 230 120 433    938:1244\n",
      "8.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 234 556 120 230 120    1324:807\n",
      "6.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 383 120 230 120 186    1460:994\n"
     ]
    }
   ],
   "source": [
    "def check_enc_dec(file_pattern, enc, dec):\n",
    "    \n",
    "    filenames = list(tf.data.Dataset.list_files(file_pattern, seed=10).as_numpy_iterator())\n",
    "    filenames = [fn.decode('utf-8') for fn in filenames]\n",
    "\n",
    "    def embedding_values(e):\n",
    "        return np.squeeze(e)\n",
    "            \n",
    "    enc_values = embedding_values(enc)\n",
    "    enc_values = [np.flip(xs) for xs in enc_values]\n",
    "    enc_values = [list(xs.astype(str)) for xs in enc_values]\n",
    "    \n",
    "    dec_values = embedding_values(dec)\n",
    "    dec_values = [np.flip(xs) for xs in dec_values]\n",
    "    dec_values = [list(xs.astype(str)) for xs in dec_values]\n",
    "\n",
    "    enc_lengths = get_sequence_lengths(enc)\n",
    "    dec_lengths = get_sequence_lengths(dec)\n",
    "    \n",
    "    print('Values:')\n",
    "    print('Filename                                      First few bytes                                  lengths')\n",
    "    for i, filename in enumerate(filenames):\n",
    "        fn = filename.split(os.sep)[-1]\n",
    "        print('{}: {}:{}    {}:{}'.format(fn, ' '.join(enc_values[i][:10]), ' '.join(dec_values[i][:10]),\n",
    "                                          int(enc_lengths[i]), int(dec_lengths[i])))\n",
    "\n",
    "check_enc_dec('/Volumes/Seagate/generated-data-combined-html-json/*.combined',\n",
    "              encoder_values, decoder_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.008861Z",
     "start_time": "2020-10-13T01:44:20.985266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename                                      First few bytes                                  lengths\n",
      "0.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 420 120 230 120 528    1322:805\n",
      "1.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 356 120 230 120 264    1467:1001\n",
      "2.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 180 120 230 120 322    1321:804\n",
      "3.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 417 473 120 230 120    1460:994\n",
      "4.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 513 120 230 120 191    1456:990\n",
      "5.combined: 263 293 217 253 569 570 366 156 521 313:306 152 368 402 120 313 120 230 120 433    938:1244\n",
      "6.combined: 263 293 217 253 569 378 269 253 366 156:306 152 368 402 120 383 120 230 120 186    1460:994\n",
      "7.combined: 263 293 217 253 569 570 366 156 521 298:306 152 368 402 120 298 509 120 230 120    938:1244\n",
      "8.combined: 263 293 237 217 237 492 308 492 308 492:306 152 368 402 120 234 556 120 230 120    1324:807\n",
      "9.combined: 263 293 217 492 237 369 470 308 492 237:306 152 368 402 120 369 470 120 230 120    2408:1451\n"
     ]
    }
   ],
   "source": [
    "def check_data_files(file_pattern):\n",
    "    enc_lengths = dec_lengths = []\n",
    "    print('Filename                                      First few bytes                                  lengths')\n",
    "    for i, fn in enumerate(iglob(file_pattern)):\n",
    "        with open(fn, 'r') as f:\n",
    "            line = f.read()\n",
    "        parts = line.split(':')\n",
    "        values = [xs.split() for xs in parts]\n",
    "        values = [[str(x) for x in xs] for xs in values]\n",
    "        enc_len, dec_len = [len(x) for x in values]\n",
    "        enc_lengths.append(enc_len)\n",
    "        dec_lengths.append(dec_len)\n",
    "        filename = fn.split(os.sep)[-1]\n",
    "        \n",
    "        print(f'{filename}: {\" \".join(values[0][:10])}:{\" \".join(values[1][:10])}    {enc_len}:{dec_len}')\n",
    "        \n",
    "check_data_files('/Volumes/Seagate/generated-data-combined-html-json/*.combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.079572Z",
     "start_time": "2020-10-13T01:44:21.012636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 938., 1322., 1456., 1467., 2408., 1321., 1460.,  938., 1324.,\n",
       "        1460.]),\n",
       " array([1244.,  805.,  990., 1001., 1451.,  804.,  994., 1244.,  807.,\n",
       "         994.]))"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence_lengths(encoder_values), get_sequence_lengths(decoder_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.087320Z",
     "start_time": "2020-10-13T01:44:21.082489Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [237],\n",
       "         [293],\n",
       "         [263]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [217],\n",
       "         [293],\n",
       "         [263]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(10, 2408, 1), dtype=int32, numpy=\n",
       " array([[[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]],\n",
       " \n",
       "        [[  0],\n",
       "         [  0],\n",
       "         [  0],\n",
       "         ...,\n",
       "         [368],\n",
       "         [152],\n",
       "         [306]]], dtype=int32)>)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_values, decoder_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.128630Z",
     "start_time": "2020-10-13T01:44:21.090398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(882,\n",
       " [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  97,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  1434,\n",
       "  1595,\n",
       "  1814,\n",
       "  1872,\n",
       "  3101,\n",
       "  4218,\n",
       "  4952,\n",
       "  5493,\n",
       "  5706,\n",
       "  6743,\n",
       "  6839,\n",
       "  7226,\n",
       "  8244,\n",
       "  8772,\n",
       "  9235,\n",
       "  9482,\n",
       "  9621,\n",
       "  10726,\n",
       "  10783,\n",
       "  11082,\n",
       "  11808,\n",
       "  11962,\n",
       "  13020,\n",
       "  13126,\n",
       "  13576,\n",
       "  13816,\n",
       "  13832,\n",
       "  14192,\n",
       "  14234,\n",
       "  15114,\n",
       "  15161,\n",
       "  15299,\n",
       "  15436,\n",
       "  15644,\n",
       "  16992,\n",
       "  17213,\n",
       "  17355,\n",
       "  18004,\n",
       "  18006,\n",
       "  18169,\n",
       "  18533,\n",
       "  18904,\n",
       "  19292,\n",
       "  20031,\n",
       "  20146,\n",
       "  20977,\n",
       "  21573,\n",
       "  22292,\n",
       "  22419,\n",
       "  22428,\n",
       "  22817,\n",
       "  22847,\n",
       "  24303,\n",
       "  24929,\n",
       "  25644,\n",
       "  25770,\n",
       "  26328,\n",
       "  26372,\n",
       "  26619,\n",
       "  27599,\n",
       "  27820,\n",
       "  29461,\n",
       "  30270,\n",
       "  30276,\n",
       "  30391,\n",
       "  30624,\n",
       "  30709,\n",
       "  30783,\n",
       "  31038,\n",
       "  31498,\n",
       "  31828,\n",
       "  31846,\n",
       "  32376,\n",
       "  32499,\n",
       "  33051,\n",
       "  33444,\n",
       "  33627,\n",
       "  34170,\n",
       "  35716,\n",
       "  36213,\n",
       "  36375,\n",
       "  36582,\n",
       "  36919,\n",
       "  37557,\n",
       "  37928,\n",
       "  38295,\n",
       "  38898,\n",
       "  39062,\n",
       "  39272,\n",
       "  40242,\n",
       "  40277,\n",
       "  40296,\n",
       "  41099,\n",
       "  41130,\n",
       "  42327,\n",
       "  42498,\n",
       "  42921,\n",
       "  43274,\n",
       "  44285,\n",
       "  44927,\n",
       "  45641,\n",
       "  46741,\n",
       "  47250,\n",
       "  47629,\n",
       "  47834,\n",
       "  47983,\n",
       "  48081,\n",
       "  48306,\n",
       "  48403,\n",
       "  48937,\n",
       "  49097,\n",
       "  49438,\n",
       "  49588,\n",
       "  49629,\n",
       "  49916,\n",
       "  49919,\n",
       "  50504,\n",
       "  50582,\n",
       "  51212,\n",
       "  51399,\n",
       "  51554,\n",
       "  51590,\n",
       "  52090,\n",
       "  52406,\n",
       "  52773,\n",
       "  52876,\n",
       "  53710,\n",
       "  54220,\n",
       "  54792,\n",
       "  55210,\n",
       "  55881,\n",
       "  55915,\n",
       "  56321,\n",
       "  56384,\n",
       "  57149,\n",
       "  57420,\n",
       "  57525,\n",
       "  59092,\n",
       "  59095,\n",
       "  59842,\n",
       "  60072,\n",
       "  60569,\n",
       "  61027,\n",
       "  61123,\n",
       "  61317,\n",
       "  61322,\n",
       "  61971,\n",
       "  62346,\n",
       "  62606,\n",
       "  63236,\n",
       "  63532,\n",
       "  63732,\n",
       "  63942,\n",
       "  64564,\n",
       "  64604,\n",
       "  65421,\n",
       "  65743,\n",
       "  66375,\n",
       "  67093,\n",
       "  67166,\n",
       "  67583,\n",
       "  68141,\n",
       "  68347,\n",
       "  68434,\n",
       "  68933,\n",
       "  69181,\n",
       "  69319,\n",
       "  70219,\n",
       "  70247,\n",
       "  71429,\n",
       "  71656,\n",
       "  72128,\n",
       "  72498,\n",
       "  72534,\n",
       "  74112,\n",
       "  74439,\n",
       "  74803,\n",
       "  75812,\n",
       "  75953,\n",
       "  76468,\n",
       "  76915,\n",
       "  77886,\n",
       "  78601,\n",
       "  78839,\n",
       "  79132,\n",
       "  79177,\n",
       "  79366,\n",
       "  79425,\n",
       "  80428,\n",
       "  81333,\n",
       "  81842,\n",
       "  81844,\n",
       "  82204,\n",
       "  82482,\n",
       "  82891,\n",
       "  83347,\n",
       "  83439,\n",
       "  83997,\n",
       "  84442,\n",
       "  84469,\n",
       "  84581,\n",
       "  84615,\n",
       "  84790,\n",
       "  85196,\n",
       "  85198,\n",
       "  86279,\n",
       "  86306,\n",
       "  86457,\n",
       "  86749,\n",
       "  87082,\n",
       "  87811,\n",
       "  88422,\n",
       "  88488,\n",
       "  88512,\n",
       "  88543,\n",
       "  89991,\n",
       "  90510,\n",
       "  90789,\n",
       "  90937,\n",
       "  91075,\n",
       "  91504,\n",
       "  92111,\n",
       "  92165,\n",
       "  93873,\n",
       "  94370,\n",
       "  95123,\n",
       "  96458,\n",
       "  96510,\n",
       "  96527,\n",
       "  96727,\n",
       "  97032,\n",
       "  97318,\n",
       "  97482,\n",
       "  99258,\n",
       "  99458,\n",
       "  99716,\n",
       "  122093,\n",
       "  132641,\n",
       "  148418,\n",
       "  177557,\n",
       "  187515,\n",
       "  227301,\n",
       "  234900,\n",
       "  238882,\n",
       "  270938,\n",
       "  272016,\n",
       "  300755,\n",
       "  316927,\n",
       "  338596,\n",
       "  339857,\n",
       "  340324,\n",
       "  369204,\n",
       "  377509,\n",
       "  378195,\n",
       "  381169,\n",
       "  386498,\n",
       "  403410,\n",
       "  407448,\n",
       "  409482,\n",
       "  437120,\n",
       "  453893,\n",
       "  455201,\n",
       "  460872,\n",
       "  461544,\n",
       "  476401,\n",
       "  503677,\n",
       "  545342,\n",
       "  585967,\n",
       "  589781,\n",
       "  630399,\n",
       "  638422,\n",
       "  643768,\n",
       "  653019,\n",
       "  655395,\n",
       "  663332,\n",
       "  712355,\n",
       "  769358,\n",
       "  776199,\n",
       "  778196,\n",
       "  789895,\n",
       "  790845,\n",
       "  796301,\n",
       "  799889,\n",
       "  858877,\n",
       "  878891,\n",
       "  895874,\n",
       "  896261,\n",
       "  952349,\n",
       "  952813,\n",
       "  963810,\n",
       "  977999,\n",
       "  990350,\n",
       "  1003012,\n",
       "  1619135,\n",
       "  2244298,\n",
       "  3395670,\n",
       "  3854652,\n",
       "  4566181,\n",
       "  6201368,\n",
       "  7505635,\n",
       "  7907276,\n",
       "  8655107,\n",
       "  8749571,\n",
       "  9129829,\n",
       "  999999999])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocabulary(enc, dec):\n",
    "    def build_vocab(values):\n",
    "        values_set = set()        \n",
    "        for v in values:\n",
    "            values_set.update(np.squeeze(v))\n",
    "        return values_set\n",
    "    \n",
    "    enc_set = build_vocab(enc)\n",
    "    dec_set = build_vocab(dec)\n",
    "    values_set = enc_set | dec_set\n",
    "    \n",
    "    return sorted(list(values_set))\n",
    "\n",
    "encoder_values = np.squeeze(encoder_values)\n",
    "decoder_values = np.squeeze(decoder_values)\n",
    "vocab = build_vocabulary(encoder_values, decoder_values)\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 4  # 4 float32 values for each token of input\n",
    "vocab_size, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:21.136886Z",
     "start_time": "2020-10-13T01:44:21.130856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (10, 2408), dtype('int32'))"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder_values), encoder_values.shape, encoder_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T01:44:25.197352Z",
     "start_time": "2020-10-13T01:44:21.140581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10, 2408)\n",
      "<class 'numpy.ndarray'> (10, 2408)\n"
     ]
    }
   ],
   "source": [
    "vocab_array = np.array(vocab)\n",
    "\n",
    "def build_indices(values):\n",
    "    return np.squeeze(np.array([[np.where(vocab == x) \n",
    "                                    for x in value] \n",
    "                                for value in values]))\n",
    "\n",
    "encoder_indices = build_indices(encoder_values)\n",
    "decoder_indices = build_indices(decoder_values)\n",
    "print(type(encoder_indices), encoder_indices.shape)\n",
    "print(type(decoder_indices), decoder_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:14:15.052669Z",
     "start_time": "2020-10-13T02:14:14.714647Z"
    }
   },
   "outputs": [],
   "source": [
    "# All of this code is taken from Aurelien Geron's\n",
    "# notebook which accompanies the book\n",
    "# Handson Machine Learning with Scikit-Learn and Tensorflow.\n",
    "# You can find it here:\n",
    "# https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb\n",
    "#\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(4, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(4)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:14:15.861155Z",
     "start_time": "2020-10-13T02:14:15.849190Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"adam\",\n",
    "              run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:14:16.960096Z",
     "start_time": "2020-10-13T02:14:16.953741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_60\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_154 (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_153 (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_31 (Embedding)        (None, None, 4)      3528        input_153[0][0]                  \n",
      "                                                                 input_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                  [(None, 4), (None, 4 144         embedding_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_155 (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_35 (BasicDecoder) (BasicDecoderOutput( 4554        embedding_31[1][0]               \n",
      "                                                                 lstm_61[0][1]                    \n",
      "                                                                 lstm_61[0][2]                    \n",
      "                                                                 input_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_32 (TensorF [(None, None, 882)]  0           basic_decoder_35[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 8,226\n",
      "Trainable params: 8,226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T02:16:03.058703Z",
     "start_time": "2020-10-13T02:15:43.880120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7763\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad310eba00>"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_indices_shifted = np.c_[np.zeros((decoder_indices.shape[0], 1)),\n",
    "                                decoder_indices[:, :-1]]\n",
    "# print(encoder_indices.shape)\n",
    "# print(decoder_indices.shape)\n",
    "sequence_lengths = np.full([decoder_indices.shape[0]], decoder_indices.shape[1])\n",
    "# print(sequence_lengths.shape)\n",
    "# print(sequence_lengths[:5])\n",
    "# print(type(sequence_lengths))\n",
    "model.fit([encoder_indices, decoder_indices_shifted, sequence_lengths], \n",
    "          decoder_indices,\n",
    "          epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "544.85px",
    "left": "1016.8px",
    "right": "20px",
    "top": "120px",
    "width": "259.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
