{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.152215Z",
     "start_time": "2020-10-07T17:22:34.731220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.164513Z",
     "start_time": "2020-10-07T17:22:38.154633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.185024Z",
     "start_time": "2020-10-07T17:22:38.167476Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import iglob\n",
    "\n",
    "html_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/html/encoded/*.unescaped.encoded')))\n",
    "json_filenames = sorted(list(iglob('/Volumes/Seagate/generated-data/expected_json/encoded/*.expected_json.encoded')))\n",
    "assert(len(html_filenames) == len(json_filenames))\n",
    "combined_filenames = zip(html_filenames, json_filenames)\n",
    "\n",
    "for html_fn, json_fn in combined_filenames:\n",
    "    # print(html_fn, '\\n\\t', json_fn)\n",
    "    with open(html_fn, 'r') as f:\n",
    "        html_data = f.read()\n",
    "    with open(json_fn, 'r') as f:\n",
    "        json_data = f.read()\n",
    "\n",
    "    with open(os.path.join('/Volumes/Seagate/generated-data-combined-html-json',\n",
    "                           html_fn.split(os.sep)[-1].split('.')[0] + '.combined'), 'w') as f:\n",
    "        f.write(html_data + ' : ' + json_data)\n",
    "\n",
    "\n",
    "def read_file(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def write_file(fn, data):\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    write_file(dst, read_file(src))\n",
    "    \n",
    "\n",
    "copy_file('/Volumes/Seagate/generated-data/expected_json/encoded/max_encoded_file_token_len',\n",
    "          '/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:22:38.190812Z",
     "start_time": "2020-10-07T17:22:38.187535Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/Seagate/generated-data-combined-html-json/max_encoded_file_token_len', 'r') as f:\n",
    "    line = f.read()\n",
    "    key, value = line.split('=')\n",
    "    assert(key == 'max_encoded_file_token_len')\n",
    "    max_encoded_file_token_len = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:27:00.715145Z",
     "start_time": "2020-10-07T17:27:00.707907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle the data:\n",
    "#   - During training:\n",
    "#     - We're planning on using 10K generated files.\n",
    "#       Average file size around 9K\n",
    "#       90M X 4 (for uint32 numbers) = 360MB full HTML training data.\n",
    "#       Also some more memory needed to hold JSON training data.\n",
    "#       We can decrease from 10K files to 5K generated files, \n",
    "#       or increase the memory reserved for this application to\n",
    "#       hold this entire data in memory.\n",
    "#     - So we can shuffle this data as a part of the model.\n",
    "#       It is good to shuffle at least per epoch so the model\n",
    "#       is not biased.\n",
    "#     - You can specify:\n",
    "#       dataset = dataset.shuffle(buffer_size=100,    # prefilled buffer to speed up shuffling\n",
    "#                                 random_seed = 10,   # random seed set to ensure repeatability\n",
    "#                                 reshuffle_each_iteration=True)  # True by default. Set to False for debugging.\n",
    "#   - During validation/testing:\n",
    "#     - No need to hold the entire dataset in memory to do this since\n",
    "#       we can apply the model for validation testing on each file.\n",
    "def get_datasets(filepath):\n",
    "    def get_text_line_dataset(filepath):\n",
    "       return tf.data.TextLineDataset(filepath)\n",
    "\n",
    "    def get_combined(line):\n",
    "        print(type(line))\n",
    "        return tf.strings.split(line, ':')\n",
    "    \n",
    "    def unicode_to_ascii(unicode):\n",
    "        return tf.strings.to_number(unicode, out_type=tf.int32)\n",
    "\n",
    "    def pad(ints):\n",
    "        print(type(ints))\n",
    "        t = ints.to_tensor(shape=(2, max_encoded_file_token_len))\n",
    "        return t\n",
    "\n",
    "    def reverse(padded):\n",
    "        print(type(padded))\n",
    "        return tf.reverse(padded, axis=[1])\n",
    "    \n",
    "    n_readers = 5\n",
    "    dataset = tf.data.Dataset.list_files(filepath, seed=10) \\\n",
    "                             .interleave(get_text_line_dataset, cycle_length=n_readers) \\\n",
    "                             .map(get_combined) \\\n",
    "                             .map(tf.strings.split) \\\n",
    "                             .map(unicode_to_ascii) \\\n",
    "                             .map(int) \\\n",
    "                             .map(pad) \\\n",
    "                             .map(reverse)\n",
    "\n",
    "    #for x in dataset:\n",
    "    #    print(x)\n",
    "    #    break\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:27:01.684342Z",
     "start_time": "2020-10-07T17:27:01.681861Z"
    }
   },
   "outputs": [],
   "source": [
    "# At the end, we will batch and prefetch\n",
    "# return dataset.batch(batch_size).prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:27:02.782720Z",
     "start_time": "2020-10-07T17:27:02.569831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "combined_ds = get_datasets('/Volumes/Seagate/generated-data-combined-html-json/*.combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:38:28.574588Z",
     "start_time": "2020-10-07T17:38:28.474950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  0   0   0 ... 237 293 263]\n",
      " [  0   0   0 ... 368 152 306]], shape=(2, 2408), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x in combined_ds.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:49:13.520288Z",
     "start_time": "2020-10-07T17:49:13.515774Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_len(ds):\n",
    "    cardinality = tf.data.experimental.cardinality(ds)\n",
    "    if cardinality == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "        print('INFINITE_CARDINALITY')\n",
    "        return\n",
    "    elif cardinality < 0:\n",
    "        print(f'Negative cardinality: {cardinality}')\n",
    "        \n",
    "    count = 0\n",
    "    for x in combined_ds:\n",
    "        count += 1\n",
    "    print(f'Counted dataset length: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:49:14.510690Z",
     "start_time": "2020-10-07T17:49:14.411613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative cardinality: -2\n",
      "Counted dataset length: 10\n"
     ]
    }
   ],
   "source": [
    "dataset_len(combined_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:44:34.314340Z",
     "start_time": "2020-10-05T20:44:34.303531Z"
    }
   },
   "outputs": [],
   "source": [
    "text_vectorizer = \\\n",
    "    preprocessing.TextVectorization(max_tokens=None, standardize=None,\n",
    "                                   split=\"whitespace\", ngrams=None,\n",
    "                                   output_mode=\"int\", output_sequence_length=max_data_len,\n",
    "                                   pad_to_max_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:44:35.131646Z",
     "start_time": "2020-10-05T20:44:35.119018Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a1da5003c095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madapted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adapted_data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "adapted_data = text_vectorizer.adapt(data.batch(64))\n",
    "print('adapted_data:', adapted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T17:58:35.916839Z",
     "start_time": "2020-10-05T17:58:35.672440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '1', '10', '11', '12,', '13,', '14,', '15,', '16', '17,', '18,', '19,', '2', '20,', '21', '3', '4', '5', '6', '7', '8', '9', '[UNK]']\n",
      "test_output: tf.Tensor([[0.0213149]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vocab = text_vectorizer.get_vocabulary()\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "test_data = tf.constant([\"12 5 9 20 52\"])\n",
    "test_output = model(test_data)\n",
    "\n",
    "print(\"test_output:\", test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T17:49:04.095432Z",
     "start_time": "2020-10-05T17:49:03.788556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'the', 'side', 'you', 'with', 'will', 'wider', 'them', 'than', 'sky', 'put', 'other', 'one', 'is', 'for', 'ease', 'contain', 'by', 'brain', 'beside', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"int\" output_mode\n",
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\")\n",
    "# Index the vocabulary via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "\n",
    "# You can retrieve the vocabulary we indexed via get_vocabulary()\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Create an Embedding + LSTM model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the model on test data (which includes unknown tokens)\n",
    "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
    "test_output = model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
